{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11815141,"sourceType":"datasetVersion","datasetId":7421066}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/hmcdataset/intraday.csv\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T21:08:29.079633Z","iopub.execute_input":"2025-05-21T21:08:29.080209Z","iopub.status.idle":"2025-05-21T21:08:30.031516Z","shell.execute_reply.started":"2025-05-21T21:08:29.080186Z","shell.execute_reply":"2025-05-21T21:08:30.030936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing\ndf['start'] = pd.to_datetime(df['start'], errors='coerce')\ndf['date'] = df['start'].dt.date\ndf['hour'] = df['start'].dt.floor('h')  # use lowercase 'h' to avoid deprecation warning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T21:08:35.499350Z","iopub.execute_input":"2025-05-21T21:08:35.499833Z","iopub.status.idle":"2025-05-21T21:08:35.622490Z","shell.execute_reply.started":"2025-05-21T21:08:35.499811Z","shell.execute_reply":"2025-05-21T21:08:35.621937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Filter relevant columns and define hypoglycemia\ndf_cgm = df[['patientID', 'date', 'hour', 'cgm', 'steps']].dropna(subset=['cgm'])\ndf_cgm['hypo'] = df_cgm['cgm'] < 70\n\n# 1. Total number of patients\ntotal_patients = df_cgm['patientID'].nunique()\n\n# 2. Average number of days per patient\ndays_per_patient = df_cgm.groupby('patientID')['date'].nunique()\navg_days_per_patient = days_per_patient.mean()\n\n# 3. Average valid CGM hours per patient (≥ 4 readings/hour)\nvalid_hours = df_cgm.groupby(['patientID', 'hour'])['cgm'].count().reset_index()\nvalid_hours = valid_hours[valid_hours['cgm'] >= 4]\nvalid_cgm_hours_per_patient = valid_hours.groupby('patientID')['hour'].count()\navg_valid_hours_per_patient = valid_cgm_hours_per_patient.mean()\n\n# 4. Hypoglycemia hours per patient\nhypo_hours_per_patient = df_cgm[df_cgm['hypo']].groupby('patientID')['hour'].nunique()\nmin_hypo = hypo_hours_per_patient.min()\nmax_hypo = hypo_hours_per_patient.max()\navg_hypo = hypo_hours_per_patient.mean()\n\n# 5. Average steps per patient (interval-based)\navg_steps_per_patient = df_cgm.groupby('patientID')['steps'].mean().mean()\n\n# 6. Average steps before hypo event (6h window)\ndf_cgm_sorted = df_cgm.sort_values(['patientID', 'hour']).set_index('hour')\ndf_cgm_sorted['prev_6h_steps'] = df_cgm_sorted.groupby('patientID')['steps'].rolling('6h').sum().reset_index(level=0, drop=True)\ndf_cgm_sorted.reset_index(inplace=True)\n\nhypo_step_window = df_cgm_sorted[df_cgm_sorted['hypo']]['prev_6h_steps']\navg_steps_before_hypo = hypo_step_window.mean()\n\n# Round results to nearest thousandth\nsummary_data = {\n    \"Metric\": [\n        \"Total Patients\",\n        \"Average Days per Patient\",\n        \"Average Valid CGM Hours per Patient (≥4/hr)\",\n        \"Min Hypo Hours per Patient\",\n        \"Max Hypo Hours per Patient\",\n        \"Average Hypo Hours per Patient\",\n        \"Average Steps per Patient (interval level)\",\n        \"Average Steps Before Hypo Event (6h window)\"\n    ],\n    \"Value\": [\n        round(total_patients, 3),\n        round(avg_days_per_patient, 3),\n        round(avg_valid_hours_per_patient, 3),\n        int(min_hypo),\n        int(max_hypo),\n        round(avg_hypo, 3),\n        round(avg_steps_per_patient, 3),\n        round(avg_steps_before_hypo, 3)\n    ]\n}\n\nsummary_df = pd.DataFrame(summary_data)\n\n# Print clean table\nprint(\"\\nRounded CGM Summary Statistics:\")\nprint(summary_df.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T21:08:45.025032Z","iopub.execute_input":"2025-05-21T21:08:45.025286Z","iopub.status.idle":"2025-05-21T21:08:45.266232Z","shell.execute_reply.started":"2025-05-21T21:08:45.025269Z","shell.execute_reply":"2025-05-21T21:08:45.265504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Aggregate to daily level: steps total, hypo % per patient per day\ndaily_summary = df_cgm.groupby(['patientID', 'date']).agg(\n    total_steps=('steps', 'sum'),\n    hypo_percent=('hypo', 'mean')\n).reset_index()\n\n# Define hypo_day if ≥ 4% CGM readings < 70\ndaily_summary['hypo_day'] = daily_summary['hypo_percent'] >= 0.04\n\n# Per-patient summary (QCRI harmonized)\nqcri_summary = daily_summary.groupby('patientID').agg(\n    total_days=('date', 'nunique'),\n    hypo_days=('hypo_day', 'sum'),\n    avg_daily_steps=('total_steps', 'mean')\n).reset_index()\n\nqcri_summary['hypo_day_percent'] = round((qcri_summary['hypo_days'] / qcri_summary['total_days']) * 100, 3)\nqcri_summary['avg_daily_steps'] = round(qcri_summary['avg_daily_steps'], 3)\n\n# Prepare validation table\nvalidation_table = pd.DataFrame({\n    \"Metric\": [\n        \"Total Days (avg)\",\n        \"% Hypoglycemic Days (range)\",\n        \"Avg. Daily Steps (range)\"\n    ],\n    \"Your Computation (Per QCRI Style)\": [\n        round(qcri_summary['total_days'].mean(), 3),\n        f\"{qcri_summary['hypo_day_percent'].min()}% to {qcri_summary['hypo_day_percent'].max()}%\",\n        f\"{qcri_summary['avg_daily_steps'].min()} to {qcri_summary['avg_daily_steps'].max()}\"\n    ]\n})\n\n\n# Prepare validation table\nvalidation_table = pd.DataFrame({\n    \"Metric\": [\n        \"Total Days (avg)\",\n        \"% Hypoglycemic Days (range)\",\n        \"Avg. Daily Steps (range)\"\n    ],\n    \"Your Computation (Per QCRI Style)\": [\n        round(qcri_summary['total_days'].mean(), 3),\n        f\"{qcri_summary['hypo_day_percent'].min()}% to {qcri_summary['hypo_day_percent'].max()}%\",\n        f\"{qcri_summary['avg_daily_steps'].min()} to {qcri_summary['avg_daily_steps'].max()}\"\n    ]\n})\n\n# Display table in console\nprint(\"\\\\nQCRI-Style Validation Table:\")\nprint(validation_table.to_string(index=False))\n\n\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress FutureWarning messages from seaborn/pandas\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Boxplot: Daily Steps on Hypo vs Non-Hypo Days\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=daily_summary, x='hypo_day', y='total_steps')\nplt.xticks([0, 1], ['Non-Hypo Day', 'Hypo Day'])\nplt.title('Boxplot: Daily Step Count on Hypo vs Non-Hypo Days')\nplt.xlabel('')\nplt.ylabel('Total Daily Steps')\nplt.tight_layout()\nplt.show()\n\n# Histogram: % Hypoglycemic Days per Patient\nplt.figure(figsize=(10, 6))\nsns.histplot(qcri_summary['hypo_day_percent'], bins=15, kde=True)\nplt.axvline(38, color='red', linestyle='--', label='QCRI Max Threshold (38%)')\nplt.title('Histogram: % of Hypoglycemic Days per Patient')\nplt.xlabel('Hypoglycemic Days (%)')\nplt.ylabel('Patient Count')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n# Create a bar plot showing each patient's percentage of hypoglycemic days\nplt.figure(figsize=(12, 6))\nsns.barplot(data=qcri_summary, x='patientID', y='hypo_day_percent', palette='viridis')\n\nplt.title('Percentage of Hypoglycemic Days per Patient')\nplt.xlabel('Patient ID')\nplt.ylabel('Hypoglycemic Days (%)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n# Extract hour from datetime and calculate hypoglycemia rate by hour of day\ndf['start'] = pd.to_datetime(df['start'], errors='coerce')\ndf['hour_of_day'] = df['start'].dt.hour\ndf['hypo'] = df['cgm'].lt(70) & df['cgm'].notna()\n\n\n# Group by hour of day and compute hypoglycemia frequency\nhourly_hypo = df.dropna(subset=['cgm']).groupby('hour_of_day')['hypo'].mean().reset_index()\n\n# Plot hypoglycemia frequency by hour of day\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=hourly_hypo, x='hour_of_day', y='hypo', marker='o')\nplt.title('Hourly Hypoglycemia Frequency Across All Patients')\nplt.xlabel('Hour of Day (0 = Midnight)')\nplt.ylabel('Hypoglycemia Rate')\nplt.xticks(range(0, 24))\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# Group by hour of day and compute CGM hypo-related statistics\nhourly_stats = df.groupby('hour_of_day').agg(\n    hypo_min=('hypo', 'min'),\n    hypo_max=('hypo', 'max'),\n    hypo_mean=('hypo', 'mean'),\n    hypo_std=('hypo', 'std')\n).reset_index()\n\n# Plot hypo stats over the day\nplt.figure(figsize=(14, 6))\nsns.lineplot(data=hourly_stats, x='hour_of_day', y='hypo_mean', label='Mean Hypo Rate', marker='D')\nsns.lineplot(data=hourly_stats, x='hour_of_day', y='hypo_std', label='Std Dev Hypo Rate', marker='^')\n\nplt.title('Hourly Hypoglycemia Frequency Across All Patients')\nplt.xlabel('Hour of Day (0 = Midnight)')\nplt.ylabel('Hypoglycemia Rate (0.01 to 0.3)')\nplt.xticks(range(0, 24))\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n# Group by hour of day and compute CGM statistics\nhourly_stats = df.groupby('hour_of_day').agg(\n    cgm_min=('cgm', 'min'),\n    cgm_max=('cgm', 'max'),\n    cgm_mean=('cgm', 'mean'),\n    cgm_std=('cgm', 'std')\n).reset_index()\n\n# Plot all on same diagram with different colors\nplt.figure(figsize=(14, 6))\nsns.lineplot(data=hourly_stats, x='hour_of_day', y='cgm_mean', label='Mean CGM', marker='D')\nsns.lineplot(data=hourly_stats, x='hour_of_day', y='cgm_std', label='Std Dev CGM', marker='^')\nsns.lineplot(data=hourly_stats, x='hour_of_day', y='cgm_min', label='min CGM', marker='^')\nplt.title('Hourly CGM Statistics Across All Patients')\nplt.xlabel('Hour of Day (0 = Midnight)')\nplt.ylabel('CGM Value (50 to 200)')\nplt.xticks(range(0, 24))\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T21:44:25.357118Z","iopub.execute_input":"2025-05-21T21:44:25.357723Z","iopub.status.idle":"2025-05-21T21:44:27.109570Z","shell.execute_reply.started":"2025-05-21T21:44:25.357700Z","shell.execute_reply":"2025-05-21T21:44:27.108811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Keep only relevant columns and drop rows without CGM\ndf_cgm = df[['patientID', 'hour', 'cgm']].dropna(subset=['cgm'])\n\n# STEP 1: Filter for complete hours (≥ 4 CGM readings)\ngrouped = df_cgm.groupby(['patientID', 'hour'])\nvalid_hours = grouped.filter(lambda x: len(x) >= 4)\n\n# STEP 2: Create features and label\nfeatures = valid_hours.groupby(['patientID', 'hour']).agg(\n    cgm_std=('cgm', 'std'),\n    cgm_min=('cgm', 'min'),\n   cgm_mean=('cgm', 'mean'),\n    cgm_max=('cgm', 'max'),\n   \n    hypo_label=('cgm', lambda x: int((x < 70).any()))\n).reset_index()\n\n# STEP 3: Sort for time-series modeling\nfeatures = features.sort_values(['patientID', 'hour']).reset_index(drop=True)\n\n# STEP 4: Display preview of the processed data\nprint(\"LSTM-ready features (preview):\")\nprint(features.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T05:58:10.348995Z","iopub.execute_input":"2025-05-15T05:58:10.349739Z","iopub.status.idle":"2025-05-15T05:58:15.286885Z","shell.execute_reply.started":"2025-05-15T05:58:10.349716Z","shell.execute_reply":"2025-05-15T05:58:15.286171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define thresholds for each CGM stat\nthresholds = {\n    'cgm_min': [60, 70, 80, 90],\n    'cgm_max': [120, 150, 180, 200],\n    'cgm_mean': [70, 90, 110, 130],\n    'cgm_std': [10, 20, 30, 40]\n}\n\n# Create a summary of hypo_label distribution by threshold\nthreshold_summary = []\n\nfor feature, values in thresholds.items():\n    for thresh in values:\n        below_thresh = features[features[feature] < thresh]\n        hypo_0 = (below_thresh['hypo_label'] == 0).sum()\n        hypo_1 = (below_thresh['hypo_label'] == 1).sum()\n        threshold_summary.append({\n            'Feature': feature,\n            'Threshold': f\"< {thresh}\",\n            'Label 0 Count': hypo_0,\n            'Label 1 Count': hypo_1\n        })\n\n# Convert to DataFrame for display\nthreshold_summary_df = pd.DataFrame(threshold_summary)\nthreshold_summary_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"cgm_min < 70 is a perfect predictor of hypoglycemia in this dataset.\n\ncgm_mean < 90 captures nearly 75% of hypoglycemic cases.\n\ncgm_std values are higher in hypoglycemic hours, indicating variability.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Use only cgm_mean without any scaling\nfeature_cols = ['cgm_mean']\nsequence_length = 24\n\nX_sequences = []\ny_labels = []\n\n# Group by patient and preserve time order\nfor patient_id, group in features.groupby('patientID'):\n    group = group.sort_values('hour').reset_index(drop=True)\n    for i in range(len(group) - sequence_length):\n        sequence = group.loc[i:i+sequence_length-1, feature_cols].values\n        label = group.loc[i + sequence_length, 'hypo_label']\n        X_sequences.append(sequence)\n        y_labels.append(label)\n\n# Convert to numpy arrays for model use\nX = np.array(X_sequences)  # shape: (samples, 6, 1)\ny = np.array(y_labels)\n\n# Confirm the shape\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)\n\n\nprint(X[:5])\nprint(y[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:55:39.961992Z","iopub.execute_input":"2025-05-21T20:55:39.962547Z","iopub.status.idle":"2025-05-21T20:55:53.245430Z","shell.execute_reply.started":"2025-05-21T20:55:39.962522Z","shell.execute_reply":"2025-05-21T20:55:53.244643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T05:58:36.573718Z","iopub.execute_input":"2025-05-15T05:58:36.573979Z","iopub.status.idle":"2025-05-15T05:58:39.723674Z","shell.execute_reply.started":"2025-05-15T05:58:36.573960Z","shell.execute_reply":"2025-05-15T05:58:39.722761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n# Augmentation: Add Gaussian noise\ndef augment_time_series(X, y):\n    noise = np.random.normal(0, 0.01, X.shape)\n    X_aug = X + noise\n    return np.vstack((X, X_aug)), np.hstack((y, y))\n\nX_train_aug, y_train_aug = augment_time_series(X_train, y_train)\n\n# Define one LSTM model (simplified from your template)\nmodel = Sequential([\n      Input(shape=(X.shape[1], X.shape[2])),  # replaces input_shape in LSTM\n    LSTM(50, return_sequences=True),\n    Dropout(0.3),\n    LSTM(25),\n    Dropout(0.3),\n    Dense(10, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_aug, y_train_aug, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n# Predict and evaluate\ny_pred_prob = model.predict(X_test).flatten()\ny_pred = (y_pred_prob >= 0.5).astype(int)\n\n# Evaluation function\ndef evaluate_classification(y_true, y_pred, model_name):\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='binary')\n    recall = recall_score(y_true, y_pred, average='binary')\n    f1 = f1_score(y_true, y_pred, average='binary')\n    auc_micro = roc_auc_score(y_true, y_pred)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n\n    print(f\"📌 {model_name} - Classification Metrics:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"AUC Micro: {auc_micro:.4f}\")\n\n    # Confusion matrix plot\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(f\"Confusion Matrix - {model_name}\")\n    plt.show()\n\n    return {\n        \"Accuracy\": accuracy,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1-Score\": f1,\n        \"AUC Micro\": auc_micro\n    }\n\n# Evaluate model\nevaluation_results = evaluate_classification(y_test, y_pred, \"LSTM_6hr\")\nevaluation_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T05:50:38.421267Z","iopub.execute_input":"2025-05-15T05:50:38.421802Z","iopub.status.idle":"2025-05-15T05:52:08.272358Z","shell.execute_reply.started":"2025-05-15T05:50:38.421773Z","shell.execute_reply":"2025-05-15T05:52:08.271715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"📊 Performance Summary\nMetric\tValue\tNotes\nAccuracy\t95.96%\tHigh overall accuracy, but misleading due to class imbalance.\nPrecision\t0.75\t75% of predicted hypoglycemic events were correct. Good! ✅\nRecall\t0.076\tOnly 7.6% of actual hypoglycemic events were detected. 🚨 Too low\nF1-Score\t0.138\tReflects poor balance between precision & recall.\nAUC (Micro)\t0.537\tJust above random guessing (0.5) — underperforming globally.\n\n🧠 Interpretation:\nYour model is too conservative — it doesn't trigger hypoglycemia detection often enough.\n\nIt misses ~92% of actual hypoglycemic events, which is a critical failure in medical contexts.\n\nHowever, when it does predict hypo, it’s fairly accurate (75% precision).\n\n","metadata":{}},{"cell_type":"code","source":"# Count the number of samples for each class in the target variable `y`\nfrom collections import Counter\n\nlabel_distribution = Counter(y)\nlabel_distribution\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T05:52:08.285218Z","iopub.execute_input":"2025-05-15T05:52:08.285413Z","iopub.status.idle":"2025-05-15T05:52:08.304868Z","shell.execute_reply.started":"2025-05-15T05:52:08.285399Z","shell.execute_reply":"2025-05-15T05:52:08.304338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n    confusion_matrix, roc_curve, precision_recall_curve, classification_report, auc\n)\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\n\n# --- Assume X and y are already defined as numpy arrays or pandas DataFrame/Series ---\n# Example placeholder:\n# X = np.random.rand(1000, 10, 1)\n# y = np.random.randint(0, 2, 1000)\n\n# --- Evaluation Function ---\ndef evaluate(y_true, y_pred, threshold):\n    print(f\"\\nThreshold: {threshold}\")\n    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n    print(f\"Precision: {precision_score(y_true, y_pred, zero_division=0):.4f}\")\n    print(f\"Recall:    {recall_score(y_true, y_pred, zero_division=0):.4f}\")\n    print(f\"F1 Score:  {f1_score(y_true, y_pred, zero_division=0):.4f}\")\n    print(f\"AUC:       {roc_auc_score(y_true, y_pred):.4f}\")\n\n# --- Augment Function ---\ndef augment(X, y):\n    noise = np.random.normal(0, 0.01, X.shape)\n    return np.vstack((X, X + noise)), np.hstack((y, y))\n\n# --- Train/Test Split ---\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\nX_train, y_train = augment(X_train, y_train)\n\n# --- Model Definitions ---\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True),\n            Dropout(0.2),\n            LSTM(50),\n            Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True),\n            Dropout(0.2),\n            LSTM(25),\n            Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(0.01)),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n\n# --- Training and Evaluation ---\nmodels = define_models((X.shape[1], X.shape[2]))\nresults = {}\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name} without FOCAL LOSS...\")\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n    y_pred_prob = model.predict(X_test).flatten()\n\n    for threshold in [0.5, 0.4, 0.3, 0.2]:\n        y_pred = (y_pred_prob >= threshold).astype(int)\n\n        acc = accuracy_score(y_test, y_pred)\n        prec = precision_score(y_test, y_pred, zero_division=0)\n        rec = recall_score(y_test, y_pred, zero_division=0)\n        f1 = f1_score(y_test, y_pred, zero_division=0)\n        auc_score = roc_auc_score(y_test, y_pred)\n        cm = confusion_matrix(y_test, y_pred)\n\n        evaluate(y_test, y_pred, threshold)\n\n        results[f\"{name}_thr_{threshold}\"] = {\n            \"Accuracy\": acc,\n            \"Precision\": prec,\n            \"Recall\": rec,\n            \"F1-Score\": f1,\n            \"AUC\": auc_score\n        }\n\n        # Plot Confusion Matrix\n        plt.figure(figsize=(5, 4))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.title(f\"Confusion Matrix - {name} @ threshold {threshold}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.tight_layout()\n        plt.show()\n\n# --- Summary Table ---\nresults_df = pd.DataFrame(results).T\nprint(\"\\nFinal Evaluation Results:\")\nprint(results_df.round(4))\n\n# --- Optimal Threshold Analysis (using last model's y_pred_prob) ---\nfpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(fpr, tpr)\nyouden_index = np.argmax(tpr - fpr)\nbest_roc_threshold = roc_thresholds[youden_index]\n\nprecision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_prob)\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\nbest_f1_index = np.argmax(f1_scores)\nbest_f1_threshold = pr_thresholds[best_f1_index]\n\n# --- Plot ROC & PR Curves ---\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')\nplt.scatter(fpr[youden_index], tpr[youden_index], color='red', label=f'Best threshold = {best_roc_threshold:.2f}')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(recall, precision, label=\"PR curve\")\nplt.scatter(recall[best_f1_index], precision[best_f1_index], color='green', label=f'Best F1 threshold = {best_f1_threshold:.2f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# --- Final Optimal Threshold Evaluation ---\nbest_threshold = best_f1_threshold\ny_pred_opt = (y_pred_prob >= best_threshold).astype(int)\n\nprint(f\"\\nBest threshold by Youden's J (ROC): {best_roc_threshold:.4f}\")\nprint(f\"Best threshold by max F1-score (PR): {best_f1_threshold:.4f}\")\n\nprint(\"\\nClassification Report at Optimal Threshold:\")\nprint(classification_report(y_test, y_pred_opt, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T04:49:50.583058Z","iopub.execute_input":"2025-05-22T04:49:50.583322Z","iopub.status.idle":"2025-05-22T05:00:49.040988Z","shell.execute_reply.started":"2025-05-22T04:49:50.583297Z","shell.execute_reply":"2025-05-22T05:00:49.040321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"📊 LSTM Model Comparison\nModel\tAccuracy\tPrecision\tRecall\tF1-Score\tAUC\nLSTM_100\t0.9598\t0.7209\t0.0871\t0.1554\t0.5428\nLSTM_50\t0.9564\t0.4746\t0.2360\t0.3152\t0.6122\nLSTM_25\t0.9575\t0.0000\t0.0000\t0.0000\t0.5000\n\n Key Insights:\nLSTM_50 has the best recall (23.6%) and F1-score (0.31) — it's the most balanced.\n\nLSTM_100 has the highest precision (72%) but recall is very low — too conservative.\n\nLSTM_25 failed to detect any positives at all. (100% false negatives)\n\nInterpretation:\nLSTM_50 is your best candidate so far for detecting hypoglycemia reliably.\n\nLSTM_100 might be useful when false alarms are costly, but it misses most real events.\n\nLSTM_25 likely over-regularized (due to L2 penalties), silencing detection entirely.\n\n \n Recommendations:\nStick with LSTM_50 — and tune it:\n\nTry reducing dropout.\n\nTrain longer (e.g., 15 epochs).\n\nUse threshold tuning (predict 1 if y_prob > 0.3 instead of 0.5).\n\nTry a Hybrid Model: LSTM_50 + rule-based flag (cgm_min < 70).\n\nPlot Precision-Recall curve or ROC AUC to visualize trade-offs.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input\n\nimport pandas as pd \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\n\ndef evaluate(y_true, y_pred, threshold):\n    print(f\"\\nThreshold: {threshold}\")\n    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n    print(f\"Precision: {precision_score(y_true, y_pred, zero_division=0):.4f}\")\n    print(f\"Recall: {recall_score(y_true, y_pred, zero_division=0):.4f}\")\n    print(f\"F1 Score: {f1_score(y_true, y_pred, zero_division=0):.4f}\")\n    print(f\"AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n\n\n\n\n\n# ✅ Focal Loss definition\ndef focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n        return alpha_t * tf.math.pow((1 - p_t), gamma) * bce\n    return loss\n\n\n# ✅ Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# ✅ Augmentation: add small noise\ndef augment(X, y):\n    noise = np.random.normal(0, 0.01, X.shape)\n    return np.vstack((X, X + noise)), np.hstack((y, y))\nX_train, y_train = augment(X_train, y_train)\n\n# ✅ Define models\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True),\n            Dropout(0.2),\n            LSTM(50),\n            Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True),\n            Dropout(0.2),\n            LSTM(25),\n            Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(0.01)),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n\n# ✅ Train and evaluate using focal loss\nmodels = define_models((X.shape[1], X.shape[2]))\nresults = {}\n\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n    y_pred_prob = model.predict(X_test).flatten()\n\n    for threshold in [0.5, 0.4, 0.3, 0.2]:\n        y_pred = (y_pred_prob >= threshold).astype(int)\n\n        acc = accuracy_score(y_test, y_pred)\n        prec = precision_score(y_test, y_pred, zero_division=0)\n        rec = recall_score(y_test, y_pred, zero_division=0)\n        f1 = f1_score(y_test, y_pred, zero_division=0)\n        auc = roc_auc_score(y_test, y_pred)\n        cm = confusion_matrix(y_test, y_pred)\n\n        print(f\"\\n{name} @ threshold {threshold}\")\n        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n\n        # Save results (if needed only once per model or per threshold)\n        results[f\"{name}_thr_{threshold}\"] = {\n            \"Accuracy\": acc,\n            \"Precision\": prec,\n            \"Recall\": rec,\n            \"F1-Score\": f1,\n            \"AUC\": auc\n        }\n\n        # Confusion Matrix\n        plt.figure(figsize=(5, 4))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.title(f\"Confusion Matrix - {name} @ {threshold}\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.tight_layout()\n        plt.show()\n\n# Summary Table\nresults_df = pd.DataFrame(results).T\nprint(\"\\nFinal Evaluation Results:\")\nprint(results_df.round(4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T23:39:21.175774Z","iopub.execute_input":"2025-05-21T23:39:21.176302Z","iopub.status.idle":"2025-05-21T23:43:19.126380Z","shell.execute_reply.started":"2025-05-21T23:39:21.176270Z","shell.execute_reply":"2025-05-21T23:43:19.125636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T03:31:29.575982Z","iopub.execute_input":"2025-05-22T03:31:29.576581Z","iopub.status.idle":"2025-05-22T03:31:29.581624Z","shell.execute_reply.started":"2025-05-22T03:31:29.576559Z","shell.execute_reply":"2025-05-22T03:31:29.580722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nfrom tensorflow.keras.regularizers import l1\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, mean_squared_error, mean_absolute_error,\n    roc_curve, precision_recall_curve, auc\n)\n\n# ✅ Focal Loss\ndef focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n        return alpha_t * tf.pow(1 - p_t, gamma) * bce\n    return loss\n\n# ✅ Train/Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# ✅ Augmentation\ndef augment(X, y):\n    noise = np.random.normal(0, 0.01, X.shape)\n    return np.vstack((X, X + noise)), np.hstack((y, y))\n\nX_train, y_train = augment(X_train, y_train)\n\n# ✅ Define Models\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True),\n            Dropout(0.2),\n            LSTM(50),\n            Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True),\n            Dropout(0.2),\n            LSTM(25),\n            Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(0.01)),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n\n# ✅ Evaluation Function\ndef evaluate_with_regression_metrics(y_true, y_pred, model_name=\"LSTM\"):\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    auc_score = roc_auc_score(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    mae = mean_absolute_error(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n\n    print(f\"\\n📊 {model_name} - Evaluation Metrics:\")\n    print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n    print(f\"F1-Score: {f1:.4f}, AUC: {auc_score:.4f}\")\n    print(f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title(f\"Confusion Matrix - {model_name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        \"Accuracy\": acc,\n        \"Precision\": prec,\n        \"Recall\": rec,\n        \"F1-Score\": f1,\n        \"AUC\": auc_score,\n        \"RMSE\": rmse,\n        \"MAE\": mae\n    }\n\n# ✅ Training and Evaluation\nmodels = define_models((X.shape[1], X.shape[2]))\nresults = {}\n\nfor name, model in models.items():\n    print(f\"\\n🚀 Training {name} with FOCAL LOSS...\")\n    model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n    y_pred_prob = model.predict(X_test).flatten()\n\n    # 📈 ROC and PR Curve Analysis (once per model)\n    fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    youden_index = np.argmax(tpr - fpr)\n    best_roc_threshold = roc_thresholds[youden_index]\n\n    precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_prob)\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n    best_f1_index = np.argmax(f1_scores)\n    best_f1_threshold = pr_thresholds[best_f1_index]\n\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')\n    plt.scatter(fpr[youden_index], tpr[youden_index], color='red', label=f'Best ROC Threshold = {best_roc_threshold:.2f}')\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(recall, precision, label='PR Curve')\n    plt.scatter(recall[best_f1_index], precision[best_f1_index], color='green', label=f'Best F1 Threshold = {best_f1_threshold:.2f}')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Best threshold by Youden's J statistic (ROC): {best_roc_threshold:.4f}\")\n    print(f\"Best threshold by max F1-score (PR): {best_f1_threshold:.4f}\")\n\n    # 📊 Evaluate using fixed thresholds\n    for threshold in [0.5, 0.4, 0.3, 0.2, best_f1_threshold]:\n        y_pred = (y_pred_prob >= threshold).astype(int)\n        metrics = evaluate_with_regression_metrics(y_test, y_pred, model_name=f\"{name} @ {threshold:.2f}\")\n        results[f\"{name}_thr_{threshold:.2f}\"] = metrics\n\n# ✅ Summary Table\nresults_df = pd.DataFrame(results).T\nprint(\"\\n✅ Final Evaluation Results:\")\nprint(results_df.round(4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T23:44:15.156215Z","iopub.execute_input":"2025-05-21T23:44:15.156516Z","iopub.status.idle":"2025-05-21T23:44:31.121581Z","shell.execute_reply.started":"2025-05-21T23:44:15.156493Z","shell.execute_reply":"2025-05-21T23:44:31.120938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\n# Count the class distribution\nfull_distribution = Counter(y)\ntrain_distribution = Counter(y_train)\ntest_distribution = Counter(y_test)\n\n# Display results clearly\nprint(\"🔢 Class Distribution Summary\")\nprint(f\"➡️ Full Dataset     : {dict(full_distribution)}\")\nprint(f\"➡️ Training Set     : {dict(train_distribution)}\")\nprint(f\"➡️ Testing Set      : {dict(test_distribution)}\")\n\n# Optionally, show class proportions\ndef class_proportions(counter):\n    total = sum(counter.values())\n    return {cls: f\"{count} ({count/total:.2%})\" for cls, count in counter.items()}\n\nprint(\"\\n📊 Class Proportions\")\nprint(f\"Full    : {class_proportions(full_distribution)}\")\nprint(f\"Train   : {class_proportions(train_distribution)}\")\nprint(f\"Test    : {class_proportions(test_distribution)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:08:01.219301Z","iopub.execute_input":"2025-05-28T07:08:01.219570Z","iopub.status.idle":"2025-05-28T07:08:01.246851Z","shell.execute_reply.started":"2025-05-28T07:08:01.219549Z","shell.execute_reply":"2025-05-28T07:08:01.245988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input\n\nfrom sklearn.utils import resample\nimport numpy as np\nfrom collections import Counter\n\n# Flatten LSTM format for under-sampling\nX_flat = X_train.reshape(X_train.shape[0], -1)\n\n# Separate the classes\nX_majority = X_flat[y_train == 0]\nX_minority = X_flat[y_train == 1]\n\ny_majority = y_train[y_train == 0]\ny_minority = y_train[y_train == 1]\n\n# Downsample majority class\nX_majority_down, y_majority_down = resample(\n    X_majority, y_majority,\n    replace=False,\n    n_samples=len(y_minority),\n    random_state=42\n)\n\n# Combine balanced data\nX_balanced = np.vstack((X_majority_down, X_minority))\ny_balanced = np.hstack((y_majority_down, y_minority))\n\n# Shuffle the balanced data\nindices = np.random.permutation(len(y_balanced))\nX_balanced = X_balanced[indices]\ny_balanced = y_balanced[indices]\n\n# Reshape back to LSTM format\nX_balanced = X_balanced.reshape(-1, X_train.shape[1], X_train.shape[2])\n\n# Report new distribution\nprint(\"✅ Balanced Class Distribution:\", Counter(y_balanced))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:13:28.606045Z","iopub.execute_input":"2025-05-28T07:13:28.606717Z","iopub.status.idle":"2025-05-28T07:13:28.629722Z","shell.execute_reply.started":"2025-05-28T07:13:28.606684Z","shell.execute_reply":"2025-05-28T07:13:28.629070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nfrom tensorflow.keras.regularizers import l1\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, mean_squared_error, mean_absolute_error,\n    roc_curve, precision_recall_curve, auc\n)\nfrom sklearn.utils import resample\nfrom collections import Counter\n\n# -------------------- Split and Augment --------------------\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\ndef augment(X, y):\n    noise = np.random.normal(0, 0.01, X.shape)\n    return np.vstack((X, X + noise)), np.hstack((y, y))\n\nX_train, y_train = augment(X_train, y_train)\n\n# -------------------- Undersample Majority Class --------------------\nX_flat = X_train.reshape(X_train.shape[0], -1)\nX_majority = X_flat[y_train == 0]\nX_minority = X_flat[y_train == 1]\ny_majority = y_train[y_train == 0]\ny_minority = y_train[y_train == 1]\n\nX_majority_down, y_majority_down = resample(\n    X_majority, y_majority,\n    replace=False,\n    n_samples=len(y_minority),\n    random_state=42\n)\n\nX_balanced = np.vstack((X_majority_down, X_minority))\ny_balanced = np.hstack((y_majority_down, y_minority))\n\n# Shuffle and reshape\nshuffle_idx = np.random.permutation(len(y_balanced))\nX_balanced = X_balanced[shuffle_idx].reshape(-1, X.shape[1], X.shape[2])\ny_balanced = y_balanced[shuffle_idx]\n\nprint(\"✅ Balanced Class Distribution:\", Counter(y_balanced))\n\n# -------------------- Define Focal Loss --------------------\ndef focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n        return alpha_t * tf.math.pow(1 - p_t, gamma) * bce\n    return loss\n\n# -------------------- Define LSTM Models --------------------\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True),\n            Dropout(0.2),\n            LSTM(50),\n            Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True),\n            Dropout(0.2),\n            LSTM(25),\n            Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(0.01)),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n# -------------------- Evaluation Function --------------------\ndef evaluate_model(y_true, y_pred, model_name=\"Model\"):\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    auc_score = roc_auc_score(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    mae = mean_absolute_error(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n\n    print(f\"\\n📊 {model_name} Performance:\")\n    print(f\"Accuracy : {acc:.4f} | Precision : {prec:.4f}\")\n    print(f\"Recall   : {rec:.4f} | F1-Score  : {f1:.4f}\")\n    print(f\"AUC      : {auc_score:.4f} | RMSE     : {rmse:.4f} | MAE : {mae:.4f}\")\n\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title(f\"Confusion Matrix - {model_name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-Score\": f1,\n        \"AUC\": auc_score, \"RMSE\": rmse, \"MAE\": mae\n    }\n\n# -------------------- Train + Evaluate --------------------\nresults = {}\nmodels = define_models((X.shape[1], X.shape[2]))\n\nfor name, model in models.items():\n    print(f\"\\n🚀 Training {name} \")\n    model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n    model.fit(X_balanced, y_balanced, epochs=15, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n    # Predictions\n    y_pred_prob = model.predict(X_test).flatten()\n    fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_prob)\n    precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n    best_threshold = pr_thresholds[np.argmax(f1_scores)]\n\n    # Evaluation at best threshold\n    for threshold in [0.5, 0.4, 0.3, 0.2, best_threshold]:\n        y_pred = (y_pred_prob >= threshold).astype(int)\n        key = f\"{name}_thr_{threshold:.2f}\"\n        results[key] = evaluate_model(y_test, y_pred, key)\n\n    # ROC & PR plots\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.4f}\")\n    plt.title(f\"ROC Curve - {name}\")\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(recall, precision)\n    plt.title(f\"Precision-Recall Curve - {name}\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.tight_layout()\n    plt.show()\n\n# -------------------- Summary Table --------------------\nresults_df = pd.DataFrame(results).T\nprint(\"\\n✅ Final Evaluation Results:\")\nprint(results_df.round(4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn==1.5.0 imbalanced-learn==0.12.0 --force-reinstall --no-deps\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U imbalanced-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U --force-reinstall --no-deps scikit-learn==1.5.0 imbalanced-learn==0.12.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --force-reinstall --no-deps scikit-learn==1.3.2 imbalanced-learn==0.11.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.combine import SMOTEENN, SMOTETomek\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.combine import SMOTEENN, SMOTETomek\nfrom sklearn.utils import resample\nfrom collections import Counter\nfrom tensorflow.keras.layers import Input\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, mean_squared_error, mean_absolute_error,\n    roc_curve, precision_recall_curve, auc\n)\n\n# -------------------- Split and Augment --------------------\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\ndef augment(X, y):\n    noise = np.random.normal(0, 0.01, X.shape)\n    return np.vstack((X, X + noise)), np.hstack((y, y))\n\nX_train, y_train = augment(X_train, y_train)\n\n# -------------------- Reshape for Resampling --------------------\nX_flat = X_train.reshape(X_train.shape[0], -1)\n\n# -------------------- SMOTEENN --------------------\nsmote_enn = SMOTEENN(random_state=42)\nX_smoteenn, y_smoteenn = smote_enn.fit_resample(X_flat, y_train)\nX_smoteenn = X_smoteenn.reshape(-1, X.shape[1], X.shape[2])\nprint(\"✅ After SMOTEENN:\", Counter(y_smoteenn))\n\n# -------------------- SMOTETomek --------------------\nsmote_tomek = SMOTETomek(random_state=42)\nX_smotetomek, y_smotetomek = smote_tomek.fit_resample(X_flat, y_train)\nX_smotetomek = X_smotetomek.reshape(-1, X.shape[1], X.shape[2])\nprint(\"✅ After SMOTETomek:\", Counter(y_smotetomek))\n\n# -------------------- Manual Undersampling --------------------\nX_majority = X_flat[y_train == 0]\nX_minority = X_flat[y_train == 1]\ny_majority = y_train[y_train == 0]\ny_minority = y_train[y_train == 1]\n\nX_majority_down, y_majority_down = resample(\n    X_majority, y_majority,\n    replace=False,\n    n_samples=len(y_minority),\n    random_state=42\n)\n\nX_manual = np.vstack((X_majority_down, X_minority))\ny_manual = np.hstack((y_majority_down, y_minority))\n\nshuffle_idx = np.random.permutation(len(y_manual))\nX_manual = X_manual[shuffle_idx].reshape(-1, X.shape[1], X.shape[2])\ny_manual = y_manual[shuffle_idx]\nprint(\"✅ After Manual Undersampling:\", Counter(y_manual))\n\n\n\n\n# -------------------- Define Focal Loss --------------------\ndef focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n        return alpha_t * tf.math.pow(1 - p_t, gamma) * bce\n    return loss\n\n# -------------------- Define LSTM Models --------------------\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True),\n            Dropout(0.2),\n            LSTM(50),\n            Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True),\n            Dropout(0.2),\n            LSTM(25),\n            Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(0.01)),\n            Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(0.01)),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n# -------------------- Evaluation Function --------------------\ndef evaluate_model(y_true, y_pred, model_name=\"Model\"):\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    auc_score = roc_auc_score(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    mae = mean_absolute_error(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n\n    print(f\"\\n📊 {model_name} Performance:\")\n    print(f\"Accuracy : {acc:.4f} | Precision : {prec:.4f}\")\n    print(f\"Recall   : {rec:.4f} | F1-Score  : {f1:.4f}\")\n    print(f\"AUC      : {auc_score:.4f} | RMSE     : {rmse:.4f} | MAE : {mae:.4f}\")\n\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title(f\"Confusion Matrix - {model_name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-Score\": f1,\n        \"AUC\": auc_score, \"RMSE\": rmse, \"MAE\": mae\n    }\n\n\n# -------------------- Training and Evaluation --------------------\nresults = {}\nmodels = define_models((X.shape[1], X.shape[2]))\n\nresampling_methods = {\n    \"SMOTEENN\": (X_smoteenn, y_smoteenn),\n    \"SMOTETomek\": (X_smotetomek, y_smotetomek),\n    \"ManualUndersample\": (X_manual, y_manual)\n}\n\n\nfor method, (X_resampled, y_resampled) in resampling_methods.items():\n    print(f\"\\n🔁 Resampling Strategy: {method}\")\n    for name, model in define_models((X.shape[1], X.shape[2])).items():\n        print(f\"\\n🚀 Training {name} with {method}\")\n        model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n        model.fit(X_resampled, y_resampled, epochs=15, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n\n\n        \n\n        y_pred_prob = model.predict(X_test).flatten()\n        fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_prob)\n        precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n        best_threshold = pr_thresholds[np.argmax(f1_scores)]\n\n        for threshold in [0.5, 0.4, 0.3, 0.2, best_threshold]:\n            y_pred = (y_pred_prob >= threshold).astype(int)\n            key = f\"{method}_{name}_thr_{threshold:.2f}\"\n            results[key] = evaluate_model(y_test, y_pred, key)\n\n        # ROC & PR plots\n        plt.figure(figsize=(12, 5))\n        plt.subplot(1, 2, 1)\n        plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.4f}\")\n        plt.title(f\"ROC Curve - {name} ({method})\")\n        plt.xlabel(\"FPR\")\n        plt.ylabel(\"TPR\")\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(recall, precision)\n        plt.title(f\"Precision-Recall Curve - {name} ({method})\")\n        plt.xlabel(\"Recall\")\n        plt.ylabel(\"Precision\")\n        plt.tight_layout()\n        plt.show()\n\n# -------------------- Summary Table --------------------\nresults_df = pd.DataFrame(results).T\nprint(\"\\n✅ Final Evaluation Results:\")\nprint(results_df.round(4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}