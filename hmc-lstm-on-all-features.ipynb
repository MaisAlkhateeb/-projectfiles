{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13047461,"sourceType":"datasetVersion","datasetId":7421066},{"sourceId":268813348,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ✅ 1. Define the libraries and upload the dataset","metadata":{}},{"cell_type":"code","source":"# Step 1: Create a new environment\n!python -m venv cleanenv\n\n# Step 2: Activate it\n# On Windows:\n!cleanenv\\Scripts\\activate\n# On Mac/Linux:\n#source cleanenv/bin/activate\n\n# Step 3: Install only what you need\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.0 imbalanced-learn==0.13.0 tensorflow==2.18.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GOOD (pick one)\nimport torch                         # PyTorch only\n# OR\nimport tensorflow as tf              # TensorFlow only\n# OR\nimport jax                           # JAX only","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import TF first so cuDNN is registered once\nimport tensorflow as tf\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy, scipy, sklearn, imblearn, tensorflow as tf\n\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"imbalanced-learn:\", imblearn.__version__)\nprint(\"tensorflow:\", tf.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom sklearn.utils import resample\nfrom collections import Counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This script creates **hourly-level dynamic features** for each patient during **Ramadan** using continuous glucose monitoring (CGM) data and lifestyle metrics (activity, sleep, physiology) from wearable devices.\nIt’s part of a preprocessing pipeline for modeling glucose behavior and hypoglycemia risk.\n\nHere’s a complete explanation 👇\n\n---\n\n## 🧩 **Overall Goal**\n\nTo transform raw timestamped CGM and wearable data into **hourly summarized features** that represent glucose dynamics, lifestyle behavior, and physiological activity during Ramadan — ready for statistical or machine-learning analysis.\n\n---\n\n## 🧭 **1️⃣ Load and Parse Data**\n\n* Loads the file:\n\n  ```\n  intraday_with_visits.csv\n  ```\n\n  which includes per-minute or per-sample CGM and Huawei sensor data.\n* Converts all timestamps (`start`, `date`) to datetime format.\n* Extracts:\n\n  * `hour` → the nearest hour (e.g., 14:00, 15:00).\n  * `hour_of_day` → the hour index (0–23).\n\n👉 *Purpose:* Prepare a unified hourly timeline for every patient.\n\n---\n\n## 📆 **2️⃣ Filter for Ramadan Period**\n\n* Keeps only data between **March 22 – April 19, 2023**.\n* Ensures the dataset includes `cgm` readings (continuous glucose values).\n* Adds a **binary flag `hypo`** = `True` when CGM ≤ 70 mg/dL (hypoglycemia reading).\n\n👉 *Purpose:* Focus analysis strictly on the fasting month, removing other phases.\n\n---\n\n## ⏱ **3️⃣ Validate Hourly Windows**\n\n* Keeps only hours with **≥4 CGM readings** to ensure data quality.\n* This removes incomplete or sparse hours.\n\n👉 *Purpose:* Guarantee each hourly feature represents stable glucose behavior.\n\n---\n\n## 📊 **4️⃣ Compute Hourly CGM Statistics**\n\nFor each patient and hour:\n\n* `cgm_min` → minimum glucose value\n* `cgm_max` → maximum glucose value\n* `cgm_mean` → mean glucose level\n* `cgm_std` → standard deviation (glucose variability)\n\nAlso adds:\n\n* `hypo_label` → `1` if any CGM reading in that hour was ≤70 mg/dL.\n\n👉 *Purpose:* Capture both variability and hypoglycemia presence within each hour.\n\n---\n\n## 🧮 **5️⃣ Composite Glucose Features**\n\nCreates two derived indicators:\n\n* `cgm_mean_plus_std`  → average + variability\n* `cgm_mean_minus_std` → average – variability\n\n👉 *Purpose:* Encode range boundaries for dynamic glucose variation.\n\n---\n\n## 🧠 **6️⃣ PCA on CGM Variables**\n\n* Runs **Principal Component Analysis (PCA)** on `[cgm_min, cgm_max, cgm_mean, cgm_std]`.\n* Extracts **3 principal components** (`pca_cgm1`, `pca_cgm2`, `pca_cgm3`).\n* Reports explained variance (usually >95%).\n\n👉 *Purpose:* Compress CGM dynamics into orthogonal, interpretable axes — summarizing glucose pattern, amplitude, and variability.\n\n---\n\n## 🏃‍♀️ **7️⃣ PCA on Lifestyle / Activity / Sleep Features**\n\n* Selects available columns:\n\n  ```\n  steps, distance, calories, heart_rate, spo2, deep, light, rem, nap, awake\n  ```\n* Averages these per hour per patient.\n* Runs PCA → extracts **3 lifestyle components**:\n\n  * `pc1_activity_energy` → overall activity/energy output\n  * `pc2_physiology` → physiological or heart-rate–related factors\n  * `pc3_sleep_rest` → rest and sleep quality indices\n* Reports explained variance ratio.\n\n👉 *Purpose:* Reduce multiple wearable signals into interpretable latent factors.\n\n---\n\n## 📑 **8️⃣ Finalize and Sort**\n\n* Orders the dataset by patient and hour.\n* Keeps only relevant feature columns:\n\n  ```\n  cgm_min, cgm_max, cgm_mean, cgm_std,\n  cgm_mean_plus_std, cgm_mean_minus_std,\n  pca_cgm1–3, pc1_activity_energy, pc2_physiology, pc3_sleep_rest, hypo_label\n  ```\n* Prints a preview of the final dataset.\n\n---\n\n## 💾 **9️⃣ Save Hourly Feature File**\n\nExports the final hourly-level dataset to:\n\n```\n/kaggle/working/dynamic_hourly_features_ramadan.csv\n```\n\nEach row now represents **one patient-hour** with fully engineered glucose and lifestyle features.\n\n---\n\n## ✅ **Summary in One Line**\n\n> This code aggregates intraday CGM and wearable sensor data into **hourly-level Ramadan features**, computing glucose statistics, detecting hypoglycemia, and summarizing glucose and lifestyle variability using **PCA-derived composite components** — producing a clean, feature-rich dataset for modeling hourly glucose dynamics during fasting.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# =========================\n# CONFIG\n# =========================\nCSV_PATH = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"  # ✅ update path if needed\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\n\n# =========================\n# STEP 0: Load & prepare data\n# =========================\ndf = pd.read_csv(CSV_PATH)\n\n# Parse timestamps\ndf[\"start\"] = pd.to_datetime(df[\"start\"], errors=\"coerce\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\ndf[\"hour\"] = df[\"start\"].dt.floor(\"h\")\ndf[\"hour_of_day\"] = df[\"start\"].dt.hour\n\n# Numeric conversion\nfor col in df.columns:\n    if col not in [\"patientID\", \"huaweiID\", \"visit_assigned\", \"period_main\", \"start\", \"date\", \"hour\", \"hour_of_day\"]:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# =========================\n# STEP 0.1: Ramadan filter\n# =========================\ndf = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n# Ensure CGM exists\nif \"cgm\" not in df.columns:\n    raise ValueError(\"❌ Dataset must include 'cgm' column.\")\ndf_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n# Hypo reading flag (<= 70 mg/dL)\ndf_cgm[\"hypo\"] = df_cgm[\"cgm\"] <= 70\n\n# =========================\n# STEP 1: Filter valid hours (≥4 CGM readings)\n# =========================\nvalid_hours = (\n    df_cgm.groupby([\"patientID\", \"hour\"])\n    .filter(lambda g: g[\"cgm\"].notna().sum() >= 4)\n)\n\n# =========================\n# STEP 2: Compute hourly CGM statistics\n# =========================\nhourly_features = (\n    valid_hours\n    .groupby([\"patientID\", \"hour_of_day\", \"hour\"], as_index=False)\n    .agg(\n        cgm_min=(\"cgm\", \"min\"),\n        cgm_max=(\"cgm\", \"max\"),\n        cgm_mean=(\"cgm\", \"mean\"),\n        cgm_std=(\"cgm\", \"std\")\n    )\n)\n\n# Hypoglycemia label per hour\nhypo_per_hour = (\n    valid_hours.groupby([\"patientID\", \"hour\"])[\"cgm\"]\n    .apply(lambda x: (x < 70).any())\n    .reset_index(name=\"hypo_label\")\n)\nhourly_features = hourly_features.merge(hypo_per_hour, on=[\"patientID\", \"hour\"], how=\"left\")\n\n# =========================\n# STEP 3: Composite CGM features\n# =========================\nhourly_features[\"cgm_mean_plus_std\"] = hourly_features[\"cgm_mean\"] + hourly_features[\"cgm_std\"]\nhourly_features[\"cgm_mean_minus_std\"] = hourly_features[\"cgm_mean\"] - hourly_features[\"cgm_std\"]\n\n# =========================\n# STEP 4: PCA on CGM stats → 3 components\n# =========================\npca_input_cgm = hourly_features[[\"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\"]].fillna(0)\npca_cgm = PCA(n_components=3, random_state=42)\ncgm_components = pca_cgm.fit_transform(pca_input_cgm)\n\nhourly_features[\"pca_cgm1\"] = cgm_components[:, 0]\nhourly_features[\"pca_cgm2\"] = cgm_components[:, 1]\nhourly_features[\"pca_cgm3\"] = cgm_components[:, 2]\n\nprint(\"CGM PCA explained variance:\", pca_cgm.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 5: PCA on lifestyle/activity/sleep features\n# =========================\nlifestyle_cols = [\"steps\", \"distance\", \"calories\", \"heart_rate\", \"spo2\",\n                  \"deep\", \"light\", \"rem\", \"nap\", \"awake\"]\nlifestyle_cols = [c for c in lifestyle_cols if c in df_cgm.columns]\n\nif lifestyle_cols:\n    lifestyle_hourly = (\n        df_cgm.groupby([\"patientID\", \"hour\"], as_index=False)[lifestyle_cols]\n        .mean()\n        .fillna(0)\n    )\n\n    # Merge lifestyle into hourly_features\n    hourly_features = hourly_features.merge(\n        lifestyle_hourly, on=[\"patientID\", \"hour\"], how=\"left\"\n    ).fillna(0)\n\n    # Run PCA\n    pca_life = PCA(n_components=3, random_state=42)\n    life_components = pca_life.fit_transform(hourly_features[lifestyle_cols])\n\n    hourly_features[\"pc1_activity_energy\"] = life_components[:, 0]\n    hourly_features[\"pc2_physiology\"] = life_components[:, 1]\n    hourly_features[\"pc3_sleep_rest\"] = life_components[:, 2]\n\n    print(\"Lifestyle PCA explained variance:\", pca_life.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 6: Finalize dataset\n# =========================\nhourly_features = hourly_features.sort_values([\"patientID\", \"hour\"]).reset_index(drop=True)\n\nDYNAMIC_FEATURES = [\n    \"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\",\n    \"cgm_mean_plus_std\", \"cgm_mean_minus_std\",\n    \"pca_cgm1\", \"pca_cgm2\", \"pca_cgm3\",\n    \"pc1_activity_energy\", \"pc2_physiology\", \"pc3_sleep_rest\"\n]\n\nprint(hourly_features[[\"patientID\", \"hour\"] + DYNAMIC_FEATURES + [\"hypo_label\"]].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:14:52.948272Z","iopub.execute_input":"2025-10-18T00:14:52.948568Z","iopub.status.idle":"2025-10-18T00:15:06.23163Z","shell.execute_reply.started":"2025-10-18T00:14:52.948549Z","shell.execute_reply":"2025-10-18T00:15:06.230766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Leak-safe All static visist and hourly Ramadan features + Balanced LSTM\n(hourly builder + sequences + training utilities)Below is a single, leak‑safe end‑to‑end script that:\n\nRobustly detects patient/date/variable/value columns (handles headers like PatientID (Huawei Data)).\n\nSplits by patient first, then fits PCA & scalers on TRAIN only.\n\nBuilds sequences with optional per‑patient static features.\n\nTrains LSTM variants with class‑weighted focal loss and optional resampling.\n\nChooses decision thresholds on the VALIDATION set (not the test set) to avoid peeking.\n\nEvaluates on test and writes plots + a summary CSV.\n\nWhat changed vs your last version\n\nAdded flexible column pickers (_pick_patient_col, _pick_date_col, …) and used them everywhere (intraday, visit, static).\n\nThresholds now picked on VAL (Youden and PR‑F1) → no test leakage.\n\nBalanced test creation returns X_stat_bal too (keeps seq+static aligned).\n\nResampling with SMOTE is skipped automatically when static input is enabled (can’t synthesize static safely).","metadata":{}},{"cell_type":"code","source":"# ==============================================\n# Leak-safe Ramadan features + Balanced LSTM\n# (hourly builder + sequences + training utilities)\n# ==============================================\nimport os, time, warnings, random, re\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\n# Paths (update for your environment)\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nSTATIC_CSV      = \"/kaggle/input/hmc-model-static-variables/outcome_static.csv\"\nVISIT_WIDE_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_wide_by_variable.csv\"\nVISIT_LONG_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_long.csv\"\n\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree.npz\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\nOUT_PLOTS_PNG   = \"/kaggle/working/combined_roc_pr_curves.png\"\n\n# Ramadan window and label definition\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0   # mg/dL\nMIN_CGM_PER_H = 4      # minimum CGM points within an hour to keep that hour\nSEQ_LEN       = 24     # sliding window length (hours)\n\n# Lifestyle candidates (if present in intraday_with_visits)\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\n# \"final master\" feature lists\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\n# Sequence features used for models (you can edit)\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",      # CGM core + CGM PCA#1\n    \"pc1_activity_energy\",                # lifestyle PCA#1 (0 if lifestyle missing)\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"  # visit features\n)\n\n# Training config\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01   # small Gaussian jitter on train (set None to disable)\nRESAMPLE_METHODS = [\n    \"none\",            # baseline (class_weight + focal)\n    \"oversample_seq\",  # duplicate minority sequences\n    \"undersample_seq\", # downsample majority sequences\n    # SMOTE-family below only when no static input is used\n    \"smote\", \"smoteenn\", \"smotetomek\"\n]\nUSE_STATIC_INPUT = True  # set False to ignore static input entirely (enables SMOTE variants safely)\n\n# --------------------\n# Utilities (robust column picking)\n# --------------------\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_global_seeds(RANDOM_STATE)\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef safe_encode_gender(series):\n    if series.dtype == \"object\":\n        return (series.str.strip().str.lower().map({\"male\":1, \"m\":1, \"female\":0, \"f\":0}))\n    return pd.to_numeric(series, errors=\"coerce\")\n\ndef split_patients(unique_pids, test_size=0.2, val_size=0.1, random_state=RANDOM_STATE):\n    train_pids, test_pids = train_test_split(unique_pids, test_size=test_size, random_state=random_state)\n    val_fraction = val_size / max(1e-9, (1.0 - test_size))\n    train_pids, val_pids = train_test_split(train_pids, test_size=val_fraction, random_state=random_state)\n    return np.array(train_pids), np.array(val_pids), np.array(test_pids)\n\ndef _normalize_date(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    return s.dt.normalize()\n\n# ---- robust column pickers ----\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef _pick_col_flex(\n    df: pd.DataFrame,\n    preferred=None,\n    required=False,\n    name=\"\",\n    must_contain_all=None,\n    any_contains=None,\n):\n    cols = list(df.columns)\n    norm_map = {c: _norm_col(c) for c in cols}\n\n    # (1) exact by case-insensitive preferred\n    if preferred:\n        lower_pref = {str(p).strip().lower(): p for p in preferred}\n        for c in cols:\n            if str(c).strip().lower() in lower_pref:\n                return c\n\n    # (2) exact by normalized preferred\n    if preferred:\n        pref_norm = {_norm_col(p): p for p in preferred}\n        for c, n in norm_map.items():\n            if n in pref_norm:\n                return c\n\n    # (3) heuristics on normalized names\n    cands = []\n    for c, n in norm_map.items():\n        ok = True\n        if must_contain_all:\n            for tok in must_contain_all:\n                if _norm_col(tok) not in n:\n                    ok = False\n                    break\n        if ok and any_contains:\n            if not any(_norm_col(tok) in n for tok in any_contains):\n                ok = False\n        if ok:\n            cands.append(c)\n    if cands:\n        # prioritize names starting with 'patientid' when looking for patient column\n        def _priority(col: str):\n            n = norm_map[col]\n            starts_pid = n.startswith(\"patientid\")\n            has_pid    = \"patientid\" in n\n            return (-(starts_pid or has_pid), len(n))\n        cands.sort(key=_priority)\n        return cands[0]\n\n    if required:\n        raise KeyError(\n            f\"Required column not found for {name}. \"\n            f\"preferred={preferred} | must_contain_all={must_contain_all} | any_contains={any_contains}. \"\n            f\"Available: {cols}\"\n        )\n    return None\n\ndef _pick_patient_col(df: pd.DataFrame) -> str:\n    preferred = [\"patientID\", \"patientId\", \"PatientID (Huawei Data)\", \"subject_id\", \"patid\", \"pid\", \"id\", \"huaweiid\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"patientID\",\n                          must_contain_all=[\"id\"], any_contains=[\"patient\",\"subject\",\"pat\",\"huawei\"])\n\ndef _pick_date_col(df: pd.DataFrame) -> str:\n    preferred = [\"date\", \"visit_date\", \"Date\", \"day\", \"timestamp\", \"Visit Date\", \"date_of_visit\", \"start\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"date\",\n                          any_contains=[\"date\",\"visit\",\"day\",\"timestamp\",\"start\"])\n\ndef _pick_variable_col(df: pd.DataFrame) -> str:\n    preferred = [\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"variable\",\n                          any_contains=[\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"])\n\ndef _pick_value_col(df: pd.DataFrame) -> str:\n    preferred = [\"value\",\"val\",\"measure_value\",\"reading\",\"amount\",\"score\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"value\",\n                          any_contains=[\"value\",\"val\",\"measurevalue\",\"reading\",\"amount\",\"score\"])\n\n# ---------------------------\n# Loaders for external files\n# ---------------------------\ndef load_static_df(static_csv=STATIC_CSV, needed=STATIC_COLS):\n    if not static_csv or not os.path.exists(static_csv):\n        print(\"⚠️ Static CSV not found; static features will be zero-filled.\")\n        return None\n    df = pd.read_csv(static_csv)\n    pid_col = _pick_patient_col(df)\n    df = df.rename(columns={pid_col:\"patientID\"})\n    keep = [\"patientID\"] + [c for c in needed if c in df.columns]\n    df = df[keep].drop_duplicates(subset=[\"patientID\"]).copy()\n    if \"Gender\" in df.columns:\n        df[\"Gender\"] = safe_encode_gender(df[\"Gender\"])\n    for c in keep:\n        if c != \"patientID\":\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    print(f\"ℹ️ static: using patientID column = '{pid_col}'\")\n    return df\n\ndef load_visit_df(visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV, needed=VISIT_COLS):\n    # Try wide first\n    if visit_wide_csv and os.path.exists(visit_wide_csv):\n        wide = pd.read_csv(visit_wide_csv)\n        pid_col  = _pick_patient_col(wide)\n        date_col = _pick_date_col(wide)\n        wide = wide.rename(columns={pid_col:\"patientID\", date_col:\"date\"})\n        wide[\"date\"] = _normalize_date(wide[\"date\"])\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"ℹ️ visit-wide: patientID='{pid_col}', date='{date_col}', kept={keep[2:]}\")\n            return wide[keep].copy()\n        else:\n            print(\"⚠️ VISIT_WIDE_CSV found but none of the needed visit columns present; will try LONG if available.\")\n\n    # Fallback: long -> pivot\n    if visit_long_csv and os.path.exists(visit_long_csv):\n        long = pd.read_csv(visit_long_csv)\n        pid_col   = _pick_patient_col(long)\n        date_col  = _pick_date_col(long)\n        var_col   = _pick_variable_col(long)\n        value_col = _pick_value_col(long)\n        long = long.rename(columns={pid_col:\"patientID\", date_col:\"date\", var_col:\"variable\", value_col:\"value\"})\n        long[\"date\"] = _normalize_date(long[\"date\"])\n        wide = (long\n                .pivot_table(index=[\"patientID\",\"date\"], columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n                .reset_index())\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"ℹ️ visit-long: patientID='{pid_col}', date='{date_col}', variables matched={keep[2:]}\")\n            return wide[keep].copy()\n        print(\"⚠️ VISIT_LONG_CSV present but none of the needed variables were found in the pivot.\")\n\n    print(\"⚠️ No usable visit CSVs found; visit features will be zero-filled.\")\n    return None\n\n# ----------------------------------------------------------------\n# Part A — Build hourly Ramadan features and leak‑safe transforms\n# ----------------------------------------------------------------\ndef build_hourly_features_with_leak_safe_transforms(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n    static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    # Load & parse intraday\n    df = pd.read_csv(in_csv)\n\n    # Robust patient column for intraday too\n    if \"patientID\" not in df.columns:\n        pid_col = _pick_patient_col(df)\n        df = df.rename(columns={pid_col: \"patientID\"})\n        print(f\"ℹ️ intraday: using patientID column = '{pid_col}'\")\n\n    # timestamps\n    start_col = \"start\" if \"start\" in df.columns else _pick_date_col(df)\n    df[start_col] = to_dt(df[start_col])\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[start_col].dt.date)\n    df[\"hour\"]       = df[start_col].dt.floor(\"h\")\n    df[\"hour_of_day\"]= df[\"hour\"].dt.hour\n\n    df = ensure_numeric(df)\n\n    # Ramadan filter\n    df = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n    # Require CGM\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"❌ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # Valid hourly windows\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # Base hourly CGM stats\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(\n                       cgm_min=(\"cgm\",\"min\"),\n                       cgm_max=(\"cgm\",\"max\"),\n                       cgm_mean=(\"cgm\",\"mean\"),\n                       cgm_std=(\"cgm\",\"std\")\n                   )\n                   .sort_values([\"patientID\",\"hour\"])\n                   .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # Hypo label\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # Lifestyle hourly means (if present)\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean().fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # Composite CGM features\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # Patient-level split (NO LEAK)\n    pids = hourly[\"patientID\"].dropna().unique()\n    train_p, val_p, test_p = split_patients(pids, test_size=test_size, val_size=val_size, random_state=random_state)\n    hourly[\"Split\"] = np.where(hourly[\"patientID\"].isin(train_p), \"train\",\n                        np.where(hourly[\"patientID\"].isin(val_p), \"val\", \"test\"))\n\n    # CGM PCA fit on TRAIN only\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"] == \"train\"\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    def _apply_cgm_pca(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n    hourly = _apply_cgm_pca(hourly)\n\n    # Lifestyle PCA fit on TRAIN only\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(\n            scal_life.transform(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        )\n        X_all = scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        Z_all = pca_life.transform(X_all)\n        hourly[\"pc1_activity_energy\"] = Z_all[:,0]\n        hourly[\"pc2_physiology\"]      = Z_all[:,1]\n        hourly[\"pc3_sleep_rest\"]      = Z_all[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    # Merge VISIT features (daily)\n    visit_df = load_visit_df(visit_wide_csv, visit_long_csv, VISIT_COLS)\n    hourly[\"date\"] = hourly[\"hour\"].dt.normalize()\n    if visit_df is not None:\n        visit_df[\"date\"] = pd.to_datetime(visit_df[\"date\"], errors=\"coerce\").dt.normalize()\n        visit_df = visit_df[(visit_df[\"date\"] >= RAMADAN_START) & (visit_df[\"date\"] <= RAMADAN_END)].copy()\n        hourly = hourly.merge(visit_df, on=[\"patientID\",\"date\"], how=\"left\")\n    for c in VISIT_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    # Merge STATIC features (per patient)\n    static_df = load_static_df(static_csv, STATIC_COLS)\n    if static_df is not None:\n        hourly = hourly.merge(static_df, on=\"patientID\", how=\"left\")\n    for c in STATIC_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    # Save hourly table\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(out_csv, index=False)\n    print(f\"✅ Saved hourly features (leak-safe) → {out_csv}\")\n\n    return hourly, (train_p, val_p, test_p)\n\n# ---------------------------------------------------------------\n# Part B — Build sequences (optionally with per-patient static)\n# ---------------------------------------------------------------\ndef build_sequences_by_split(\n    hourly, splits, seq_len=SEQ_LEN,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features=True\n):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float).fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n                if static_mat is not None and pid in static_mat.index:\n                    X_stat.append(static_mat.loc[pid].values.astype(float))\n        X_seq = np.array(X_seq); y = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    # Scale sequence features (fit on TRAIN only), and static (fit on TRAIN only)\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size==0: return X\n            n = X.shape[0]; return seq_scaler.transform(X.reshape(-1, n_f)).reshape(n, SEQ_LEN, n_f)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size>0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size==0: return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"✅ Sequences built | train={Xtr_s.shape}, val={Xva_s.shape}, test={Xte_s.shape}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\nTHR_MIN, THR_MAX = THR_MIN, THR_MAX  # keep constants visible here\n\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask])); idx = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment_with_static(X_seq, X_stat, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0:\n        return X_seq, X_stat, y\n    noise = np.random.normal(0, sigma, X_seq.shape)\n    X_seq_aug = np.vstack([X_seq, X_seq + noise])\n    y_aug     = np.hstack([y, y])\n    if X_stat is not None:\n        X_stat_aug = np.vstack([X_stat, X_stat])\n    else:\n        X_stat_aug = None\n    return X_seq_aug, X_stat_aug, y_aug\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE, return_index=False, allow_smote=True):\n    \"\"\"\n    Sequence-level resampling. If return_index=True, also returns the index mapping used so\n    static inputs can be resampled consistently. For SMOTE-family, index mapping isn't\n    meaningful; we disable SMOTE if allow_smote=False.\n    \"\"\"\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n    base_idx = np.arange(n)\n\n    if method == \"none\":\n        return (X, y, base_idx) if return_index else (X, y)\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return (X, y, base_idx) if return_index else (X, y)\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:  # undersample\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        Xr, yr = X[keep], y[keep]\n        return (Xr, yr, keep) if return_index else (Xr, yr)\n\n    # SMOTE family\n    if not allow_smote:\n        print(f\"⚠️ {method} disabled when static input is used; falling back to 'none'.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"⚠️ Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    Xf = X.reshape(n, -1)\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n\n    Xr = Xr.reshape(-1, T, F)\n    return (Xr, yr, None) if return_index else (Xr, yr)\n\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state=RANDOM_STATE):\n    \"\"\"Return balanced (by label) subsets of X_test, y_test, and X_stat (if given).\"\"\"\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0: \n        return (X_test, y_test, (None if X_stat is None else X_stat))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ------------------------------------------------------\n# Model builders (supports seq-only or seq+static)\n# ------------------------------------------------------\ndef make_model(seq_len, n_seq_f, n_stat_f=0, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n        x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x)\n        x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x);                    x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L1\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l1(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l1(1e-5))(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x);                          x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(32, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(32, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(32, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# ------------------------------------------------------\n# Training runner (VAL for threshold; TEST for final)\n# ------------------------------------------------------\ndef run_balanced_lstm_pipeline(data,\n                               arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE,\n                               results_csv=OUT_RESULTS_CSV,\n                               plots_png=OUT_PLOTS_PNG):\n    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n    os.makedirs(os.path.dirname(plots_png), exist_ok=True)\n    os.makedirs(\"checkpoints\", exist_ok=True)\n\n    Xtr, Xtr_stat, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Xva_stat, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n    Xte, Xte_stat, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"X_stat\"],  data[\"test\"][\"y\"]\n\n    # Augment train (and static if present)\n    Xtr, Xtr_stat, ytr = augment_with_static(Xtr, Xtr_stat, ytr, sigma=AUGMENT_SIGMA)\n\n    # Balanced copy of test for diagnostic plots\n    Xte_bal, yte_bal, Xte_stat_bal = make_balanced_test(Xte, yte, X_stat=Xte_stat)\n\n    results     = {}\n    roc_curves  = {}\n    pr_curves   = {}\n\n    allow_smote = (Xtr_stat is None or not USE_STATIC_INPUT)\n\n    def train_eval_one(method_name, arch_name):\n        nonlocal Xtr, ytr, Xtr_stat\n\n        # Resample TRAIN only\n        Xrs, yrs, idx_map = seq_resample(Xtr, ytr, method=method_name, random_state=random_state,\n                                         return_index=True, allow_smote=allow_smote)\n        if Xtr_stat is not None and USE_STATIC_INPUT:\n            if idx_map is None:\n                # SMOTE chosen while static present → already disabled in seq_resample\n                Xrs_stat = Xtr_stat\n            else:\n                Xrs_stat = Xtr_stat[idx_map]\n        else:\n            Xrs_stat = None\n\n        # Build model\n        seq_len, n_seq_f = Xrs.shape[1], Xrs.shape[2]\n        n_stat_f = 0 if (Xrs_stat is None or not USE_STATIC_INPUT) else Xrs_stat.shape[1]\n        model = make_model(seq_len, n_seq_f, n_stat_f=n_stat_f, arch=arch_name, lr=1e-3)\n\n        # Fit with VAL for early stopping (no peeking at test)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n        ckpt_path = f\"checkpoints/{method_name}__{arch_name}.h5\"\n        cp = ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_loss\", verbose=0)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xrs, Xrs_stat], yrs,\n                      validation_data=([Xva, Xva_stat], yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict([Xrs, Xrs_stat], verbose=0).ravel()\n            p_va  = model.predict([Xva, Xva_stat], verbose=0).ravel()\n            p_te  = model.predict([Xte, Xte_stat], verbose=0).ravel()\n            p_teB = model.predict([Xte_bal, Xte_stat_bal], verbose=0).ravel() if Xte_stat_bal is not None else model.predict(Xte_bal, verbose=0).ravel()\n        else:\n            model.fit(Xrs, yrs,\n                      validation_data=(Xva, yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict(Xrs, verbose=0).ravel()\n            p_va  = model.predict(Xva, verbose=0).ravel()\n            p_te  = model.predict(Xte, verbose=0).ravel()\n            p_teB = model.predict(Xte_bal, verbose=0).ravel()\n\n        # ---------- choose thresholds on VALIDATION (not TEST) ----------\n        try:\n            fpr_va, tpr_va, thr_roc_va = roc_curve(yva, p_va); auc_roc = auc(fpr_va, tpr_va)\n        except ValueError:\n            fpr_va, tpr_va, thr_roc_va, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden_va = tpr_va - fpr_va\n        t_roc, _ = _best_threshold_in_range(thr_roc_va, youden_va, thr_min, thr_max)\n\n        prec_va, rec_va, thr_pr_va = precision_recall_curve(yva, p_va)\n        f1s_va = 2*prec_va[:-1]*rec_va[:-1] / (prec_va[:-1]+rec_va[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr_va, f1s_va, thr_min, thr_max)\n        ap_val  = average_precision_score(yva, p_va)\n\n        # Curves (validation-based AUC/AP shown)\n        roc_curves[(method_name, arch_name)] = (fpr_va, tpr_va, auc_roc)\n        pr_curves[(method_name, arch_name)]  = (rec_va, prec_va, ap_val)\n        print(f\"📌 [{method_name}/{arch_name}] VAL thresholds → Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n\n        # Evaluate at thresholds: TRAIN / VAL / TEST / TEST (balanced)\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_va  = (p_va  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__train\"]        = evaluate_full_metrics(yrs,  yhat_tr,  p_tr)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__val\"]          = evaluate_full_metrics(yva,  yhat_va,  p_va)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__test\"]         = evaluate_full_metrics(yte,  yhat_te,  p_te)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__testBalanced\"] = evaluate_full_metrics(yte_bal, yhat_teB, p_teB)\n\n    # Loop: resampling methods × architectures\n    for METHOD in resample_methods:\n        if METHOD in {\"smote\",\"smoteenn\",\"smotetomek\"} and (data[\"train\"][\"X_stat\"] is not None and USE_STATIC_INPUT):\n            print(f\"⏭️  Skipping {METHOD} (static input enabled).\")\n            continue\n        print(f\"\\n🔁 Resampling: {METHOD} | train y-dist = {Counter(data['train']['y'])}\")\n        for ARCH in arch_list:\n            train_eval_one(METHOD, ARCH)\n\n    # --- Plots (validation curves)\n    plt.figure(figsize=(14,6))\n    # ROC\n    plt.subplot(1,2,1)\n    for (meth, arch), (fpr, tpr, auc_roc) in roc_curves.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{arch} (VAL AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (Validation)'); plt.legend(fontsize=8)\n    # PR\n    plt.subplot(1,2,2)\n    for (meth, arch), (rec, prec, ap) in pr_curves.items():\n        plt.plot(rec, prec, label=f'{meth}/{arch} (VAL AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR (Validation)'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(plots_png, dpi=300); plt.show()\n    print(f\"🖼️ Saved plots → {plots_png}\")\n\n    # --- Results CSV\n    results_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"Key\"})\n    k = results_df[\"Key\"].str.strip()\n    results_df[\"Split\"]  = np.where(k.str.endswith(\"__train\"), \"train\",\n                             np.where(k.str.endswith(\"__val\"), \"val\",\n                             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                             np.where(k.str.endswith(\"__test\"), \"test\", np.nan))))\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"]    = parts.str[0]\n    results_df[\"Model\"]     = parts.str[1]\n    results_df[\"Threshold\"] = pd.to_numeric(parts.str[2].str.replace(\"thr_\",\"\", regex=False), errors=\"coerce\")\n    results_df.round(6).to_csv(results_csv, index=False)\n    print(f\"📑 Saved results → {results_csv}\")\n\n    return results_df\n\n# --------------------\n# Run end-to-end\n# --------------------\nif __name__ == \"__main__\":\n    # A) Hourly features with leak‑safe PCA & merges\n    hourly, splits = build_hourly_features_with_leak_safe_transforms(\n        in_csv=CSV_INTRADAY_WITH_VISITS,\n        out_csv=OUT_HOURLY_CSV,\n        min_cgm_per_hour=MIN_CGM_PER_H,\n        test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n        static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n    )\n\n    # B) Sequences with visit + static (no leakage; scalers fit on TRAIN only)\n    data = build_sequences_by_split(\n        hourly, splits,\n        seq_len=SEQ_LEN,\n        seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n        static_cols=STATIC_COLS,\n        scale_features=True\n    )\n\n    # (Optional) Save arrays for later reuse\n    np.savez_compressed(\n        OUT_SEQ_NPZ,\n        Xtr=data[\"train\"][\"X_seq\"],  Xtr_stat=(data[\"train\"][\"X_stat\"] if data[\"train\"][\"X_stat\"] is not None else np.empty((0,0))),\n        ytr=data[\"train\"][\"y\"],\n        Xva=data[\"val\"][\"X_seq\"],    Xva_stat=(data[\"val\"][\"X_stat\"] if data[\"val\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yva=data[\"val\"][\"y\"],\n        Xte=data[\"test\"][\"X_seq\"],   Xte_stat=(data[\"test\"][\"X_stat\"] if data[\"test\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yte=data[\"test\"][\"y\"],\n        seq_features_used=np.array(data[\"seq_features_used\"], dtype=object),\n        static_features_used=np.array(data[\"static_features_used\"], dtype=object)\n    )\n    print(f\"💾 Saved sequences → {OUT_SEQ_NPZ}\")\n\n    # C) Train + evaluate (leak-safe thresholds chosen on VAL)\n    results_df = run_balanced_lstm_pipeline(\n        data,\n        arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n        resample_methods=RESAMPLE_METHODS,\n        thr_min=THR_MIN, thr_max=THR_MAX,\n        random_state=RANDOM_STATE,\n        results_csv=OUT_RESULTS_CSV,\n        plots_png=OUT_PLOTS_PNG\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T07:28:59.221963Z","iopub.execute_input":"2025-10-18T07:28:59.222215Z","iopub.status.idle":"2025-10-18T07:29:00.437226Z","shell.execute_reply.started":"2025-10-18T07:28:59.222196Z","shell.execute_reply":"2025-10-18T07:29:00.436174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here’s a concise read‑out of the results you uploaded in **`/mnt/data/results_summary_all.csv`** and what they mean in practice.\n\n---\n\n## What I analyzed\n\nFrom your results file I looked at, per split:\n\n* **Resampling method**: `none`, `oversample_seq`, `undersample_seq`\n* **Model**: `LSTM_100`, `BiLSTM`, `LSTM_50`\n* **Thresholds** evaluated: 0.40, 0.50, 0.60 (and the full probability curves to compute ROC‑AUC/PR‑AUC)\n* **Metrics**: Overall Accuracy/F1 (macro & weighted), per‑class Precision/Recall/F1/Specificity, plus ROC‑AUC and PR‑AUC\n\nSplits present in the file: **train**, **test**, and a **testBalanced** (your balanced diagnostic set with 1:1 class ratio).\n\n---\n\n##  Best practical setting\n\nIf the goal is **reliably catching hypoglycemia (Class 1)** with strong precision:\n\n* **Resampling**: `oversample_seq`\n* **Model**: `LSTM_100`\n* **Decision threshold**: **0.50**\n\n**On the balanced test set (diagnostic 1:1):**\n\n* **Weighted F1**: **0.801**\n* **Accuracy**: 0.801\n* **Class 1 (hypo) — Precision**: **0.956**\n* **Class 1 — Recall (Sensitivity)**: **0.642**\n* **ROC‑AUC**: 0.906\n* **PR‑AUC**: 0.903 *(note: PR‑AUC depends on prevalence; this is the balanced view)*\n\n**Confusion matrix (balanced test, approximate counts):**\n\n|            |       Pred 0 |      Pred 1 |\n| ---------- | -----------: | ----------: |\n| **True 0** | **TN = 130** |  **FP = 4** |\n| **True 1** |  **FN = 48** | **TP = 86** |\n\nThat’s **very few false alarms** (FP) while catching ~64% of hypos.\n\n**If you want a bit more sensitivity** (catch more hypos) and can tolerate a few more false positives, use **threshold = 0.40** for the same model/setup:\n\n* Class 1 **Recall** ↑ to **0.664**\n* Class 1 **Precision** → 0.935\n* Weighted F1 ≈ 0.799\n* Confusion matrix (balanced): **TN 126, FP 8, FN 45, TP 89**\n\n---\n\n## Why the “original test” looks deceptively great\n\nOn the **original (imbalanced) test**, the top rows (e.g., `none / LSTM_100 / thr 0.60`) show **very high weighted F1 (≈0.85–0.96)** but **very low Class 1 recall (~0.13–0.37)**. That’s because the dataset is dominated by Class 0, so a model that predicts negatives most of the time can look “great” overall while **missing most hypos**. This is a classic class‑imbalance effect.\n\nThat’s why your **testBalanced** view is important: it reveals how well the model actually detects positives.\n\n---\n\n## Method & architecture comparison (balanced test)\n\nTop performer per **resampling method** (sorted by Weighted F1):\n\n1. **oversample_seq + LSTM_100, thr 0.50**\n\n   * Weighted F1 **0.801**, Acc 0.801, **Precision₁ 0.956**, **Recall₁ 0.642**\n2. **undersample_seq + BiLSTM, thr 0.40**\n\n   * Weighted F1 0.792, Acc 0.792, Precision₁ 0.860, **Recall₁ 0.687** *(best sensitivity among the top)*\n3. **none + LSTM_50, thr 0.50**\n\n   * Weighted F1 0.740, Acc 0.740, Precision₁ 0.831, Recall₁ 0.642\n\n**Takeaway:**\n\n* **`LSTM_100`** is consistently the strongest backbone.\n* **Oversampling** improves **precision while retaining good recall**; **undersampling** nudges recall highest (but with more false alarms).\n* **No resampling** underperforms for the positive class.\n\n---\n\n## AUC perspective (threshold‑free)\n\n* For **oversample_seq + LSTM_100**:\n\n  * **ROC‑AUC (original test)**: ~**0.886**\n  * **PR‑AUC (original test)**: ~**0.336** (low due to class rarity; typical)\n  * **ROC‑AUC (balanced test)**: ~**0.906**\n  * **PR‑AUC (balanced test)**: ~**0.903** *(inflated by 50% prevalence; use for diagnostics only)*\n\nThe ROC‑AUCs are stable and indicate a **strong ranking ability**. PR‑AUC on the original test is more honest about the difficulty of the rare positives.\n\n---\n\n## Generalization check (same method/model/threshold across splits)\n\nFor **oversample_seq + LSTM_100 @ thr 0.50**:\n\n* **Train** Weighted F1 ≈ **0.972**\n* **Val** Weighted F1 ≈ **0.946**\n* **Test (original)** Weighted F1 ≈ **0.950**\n* **TestBalanced** Weighted F1 ≈ **0.801**\n\nThe drop on **testBalanced** is expected because the class prior is forced to 50/50; it does **not** indicate overfitting. Train/Val/Test are tightly aligned.\n\n---\n\n## Recommendations\n\n1. **Deploy default:** `oversample_seq + LSTM_100` with **threshold = 0.50**\n\n   * Great precision on hypos (few false alarms) with reasonable sensitivity.\n\n2. **Sensitivity mode:** set **threshold = 0.40**\n\n   * Use when **missing a hypo is costlier** than an extra false alert.\n\n3. **If you want even more recall**, consider `undersample_seq + BiLSTM @ thr 0.40` (Recall₁ ≈ **0.687** on balanced test), but expect more false positives.\n\n4. **Calibrate to clinical preference:** You can choose threshold by optimizing **Fβ** (e.g., β=2 for recall‑heavy) on the **validation set**, then lock that threshold for the test/deployment.\n\n5. **Next steps to squeeze more recall without losing precision:**\n\n   * Try adding **pca_cgm2/3** and **hour_of_day** to sequence features.\n   * Small **temporal dropout/label smoothing** to stabilize.\n   * **Patient‑grouped CV** to confirm robustness.\n   * **Threshold per risk period** (e.g., nocturnal vs daytime) using hour‑of‑day.\n\n---\n\n### Quick reference (best configurations)\n\n* **Best balanced overall**: `oversample_seq / LSTM_100 / thr=0.50`\n  **Weighted F1 0.801 · Acc 0.801 · Prec₁ 0.956 · Rec₁ 0.642 · ROC‑AUC 0.906 · PR‑AUC 0.903**\n\n* **More recall**: `oversample_seq / LSTM_100 / thr=0.40`\n  **Weighted F1 0.799 · Acc 0.799 · Prec₁ 0.935 · Rec₁ 0.664**\n\n* **Highest recall among top‑2**: `undersample_seq / BiLSTM / thr=0.40`\n  **Weighted F1 0.792 · Acc 0.792 · Prec₁ 0.860 · Rec₁ 0.687**\n\nIf you want, I can generate a compact leaderboard table (or plots) from this file showing the top N runs for each split and highlight the trade‑offs between precision and recall.\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# Results analysis & reporting\n# ============================\nimport os\nimport io\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom sklearn.metrics import (\n    confusion_matrix, roc_curve, precision_recall_curve, auc, average_precision_score\n)\n\nimport tensorflow as tf\n\n# ---------- Configuration ----------\nRESULTS_CSV_CANDIDATES = [\n    \"/mnt/data/results_summary_all (9).csv\",\n    \"/mnt/data/results_summary_all.csv\",\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\nSEQ_NPZ_CANDIDATES = [\n    \"/kaggle/working/sequences_leakfree.npz\",\n    \"/mnt/data/sequences_leakfree.npz\",\n    \"sequences_leakfree.npz\"\n]\nCHECKPOINT_DIR = \"checkpoints\"  # expects files like: checkpoints/{Method}__{Model}.h5\n\n# Your canonical visit/static lists (for feature breakdown)\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n# Which methods to summarize\nMETHODS_FOR_TOP5 = [\"none\", \"oversample_seq\", \"undersample_seq\"]\n\n# ---------- Helpers ----------\ndef _first_existing(path_list):\n    for p in path_list:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"\n    Ensure df has Method/Model/Threshold/Split. If missing, try to parse from 'Key'.\n    \"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file is missing Method/Model/Threshold/Split and has no 'Key' column to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    # Threshold is typically in the 3rd token as 'thr_0.50'\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    # Split can be at the end of the Key; back off to explicit Split col if present\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"), \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", df.get(\"Split\", np.nan)))))\n    return df\n\ndef _round_or_none(x, nd=3):\n    try:\n        return round(float(x), nd)\n    except Exception:\n        return np.nan\n\ndef _safe_confmat_from_row(row: pd.Series):\n    \"\"\"\n    Reconstructs an integer confusion matrix from supports + recall/specificity metrics in the row.\n    Assumes:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N (specificity wrt positives decision among negatives)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n    sp1  = float(row.get(\"Class1/Specificity\", np.nan))\n\n    if any([not np.isfinite(v) for v in [s0, s1, rec1, sp1]]):\n        raise ValueError(\"Cannot reconstruct confusion matrix: missing supports/recall/specificity in row.\")\n\n    tp = int(round(rec1 * s1))\n    fn = max(0, s1 - tp)\n    tn = int(round(sp1 * s0))\n    fp = max(0, s0 - tn)\n\n    # Small adjustments to keep sums consistent\n    tn = max(0, min(tn, s0))\n    fp = s0 - tn\n    tp = max(0, min(tp, s1))\n    fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    # For loading custom-loss models\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _try_load_sequences(npz_candidates):\n    p = _first_existing(npz_candidates)\n    if not p:\n        print(\"⚠️ sequences_leakfree.npz not found. Feature counts, shapes and ROC/PR will be limited.\")\n        return None\n    npz = np.load(p, allow_pickle=True)\n    # unpack with fallbacks\n    data = {\n        \"train\": {\"X_seq\": npz[\"Xtr\"], \"y\": npz[\"ytr\"]},\n        \"val\":   {\"X_seq\": npz[\"Xva\"], \"y\": npz[\"yva\"]},\n        \"test\":  {\"X_seq\": npz[\"Xte\"], \"y\": npz[\"yte\"]},\n        \"seq_features_used\": list(npz[\"seq_features_used\"].tolist()) if \"seq_features_used\" in npz.files else [],\n        \"static_features_used\": list(npz[\"static_features_used\"].tolist()) if \"static_features_used\" in npz.files else []\n    }\n    # Optional static inputs\n    if \"Xtr_stat\" in npz.files and npz[\"Xtr_stat\"].size > 0:\n        data[\"train\"][\"X_stat\"] = npz[\"Xtr_stat\"]\n    else:\n        data[\"train\"][\"X_stat\"] = None\n\n    if \"Xva_stat\" in npz.files and npz[\"Xva_stat\"].size > 0:\n        data[\"val\"][\"X_stat\"] = npz[\"Xva_stat\"]\n    else:\n        data[\"val\"][\"X_stat\"] = None\n\n    if \"Xte_stat\" in npz.files and npz[\"Xte_stat\"].size > 0:\n        data[\"test\"][\"X_stat\"] = npz[\"Xte_stat\"]\n    else:\n        data[\"test\"][\"X_stat\"] = None\n\n    data[\"npz_path\"] = p\n    return data\n\ndef _predict_loaded_model(model, X_seq, X_stat=None):\n    # Allow models with one or two inputs\n    try:\n        if isinstance(model.input, list) or len(model.inputs) > 1:\n            if X_stat is None:\n                raise ValueError(\"Model expects static input but none provided.\")\n            preds = model.predict([X_seq, X_stat], verbose=0).ravel()\n        else:\n            preds = model.predict(X_seq, verbose=0).ravel()\n        return preds\n    except Exception as e:\n        print(f\"⚠️ Prediction failed: {e}\")\n        return None\n\n# ---------- 1) Load results ----------\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results CSV. Please update RESULTS_CSV_CANDIDATES.\")\nprint(f\"📄 Using results file: {res_path}\")\ndf = pd.read_csv(res_path)\ndf = _ensure_columns(df)\n\n# ---------- 1) Top-5 tables per method (by VAL F1_weighted) ----------\nwant_cols_map = {\n    \"Model\": \"Model\",\n    \"Split\": \"Split\",\n    \"Threshold\": \"Threshold\",\n    \"Accuracy\": \"Overall/Accuracy\",\n    \"Precision_weighted\": \"Overall/Precision_weighted\",\n    \"Recall_weighted\": \"Overall/Recall_weighted\",\n    \"F1_weighted\": \"Overall/F1_weighted\",\n    \"Precision_1\": \"Class1/Precision\",\n    \"Recall_1\": \"Class1/Recall\",\n    \"F1_1\": \"Class1/F1\",\n    \"Specificity_1\": \"Class1/Specificity\",\n    \"ROC_AUC\": \"Overall/ROC-AUC\",\n    \"PR_AUC\": \"Overall/PR-AUC\",\n    \"Brier\": \"Overall/MSE_prob\"\n}\n\ndf_val = df[df[\"Split\"].str.lower() == \"val\"].copy()\nall_top5 = []\nfor m in METHODS_FOR_TOP5:\n    sub = df_val[df_val[\"Method\"] == m].copy()\n    sub = sub.dropna(subset=[\"Overall/F1_weighted\"])\n    sub = sub.sort_values(\"Overall/F1_weighted\", ascending=False).head(5)\n    if sub.empty:\n        continue\n    out = pd.DataFrame({\n        k: sub[v].values if v in sub.columns else np.nan\n        for k, v in want_cols_map.items()\n    })\n    out.insert(0, \"Method\", m)\n    all_top5.append(out)\n\ntop5_df = pd.concat(all_top5, ignore_index=True) if all_top5 else pd.DataFrame(columns=[\"Method\"]+list(want_cols_map.keys()))\ntop5_df_rounded = top5_df.copy()\nfor c in [\"Threshold\",\"Accuracy\",\"Precision_weighted\",\"Recall_weighted\",\"F1_weighted\",\n          \"Precision_1\",\"Recall_1\",\"F1_1\",\"Specificity_1\",\"ROC_AUC\",\"PR_AUC\",\"Brier\"]:\n    if c in top5_df_rounded.columns:\n        top5_df_rounded[c] = top5_df_rounded[c].apply(lambda x: _round_or_none(x, 4))\n\nprint(\"\\n=== Top-5 by VAL F1_weighted for each method (none / oversample_seq / undersample_seq) ===\")\nprint(top5_df_rounded.to_string(index=False))\n\n# Save\ntop5_out_path = \"/kaggle/working/top5_summary_per_method.csv\" if os.path.exists(\"/kaggle/working\") else \"top5_summary_per_method.csv\"\ntop5_df_rounded.to_csv(top5_out_path, index=False)\nprint(f\"\\n💾 Saved top-5 summary → {top5_out_path}\")\n\n# ---------- 2) Confusion matrix for BEST VAL F1 model ----------\nif df_val.empty:\n    print(\"\\n⚠️ No validation rows found; cannot select best VAL F1 model.\")\nelse:\n    best_row = df_val.sort_values(\"Overall/F1_weighted\", ascending=False).iloc[0]\n    cm = _safe_confmat_from_row(best_row)\n    print(\"\\n=== Best VAL model (by F1_weighted) ===\")\n    print(f\"Method={best_row['Method']} | Model={best_row['Model']} | thr={best_row['Threshold']:.2f}\")\n    print(\"Confusion Matrix [VAL]:\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Plot & save PNG\n    fig, ax = plt.subplots(figsize=(4.5, 4))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(f\"Confusion Matrix (VAL)\\n{best_row['Method']} / {best_row['Model']} @ thr={best_row['Threshold']:.2f}\")\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    # text\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=12, color=(\"white\" if cm[i,j]>cm.max()/2 else \"black\"))\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    cm_png_path = \"/kaggle/working/confusion_matrix_best_val.png\" if os.path.exists(\"/kaggle/working\") else \"confusion_matrix_best_val.png\"\n    plt.savefig(cm_png_path, dpi=200)\n    plt.close(fig)\n    print(f\"🖼️ Saved confusion matrix PNG → {cm_png_path}\")\n\n# ---------- 3) ROC/PR curves for best 5 (by VAL F1) ----------\n# We will try to load the checkpoints and the sequences NPZ.\nseq_data = _try_load_sequences(SEQ_NPZ_CANDIDATES)\nif seq_data is None:\n    print(\"\\n⚠️ Skipping ROC/PR (sequences NPZ not found).\")\nelse:\n    # pick top 5 by VAL F1 overall (unique Method/Model)\n    top5_overall = (df_val\n                    .dropna(subset=[\"Overall/F1_weighted\"])\n                    .sort_values(\"Overall/F1_weighted\", ascending=False))\n    # keep first occurrence per (Method, Model)\n    top5_overall = top5_overall.drop_duplicates(subset=[\"Method\",\"Model\"]).head(5)\n    if top5_overall.empty:\n        print(\"\\n⚠️ No candidates for ROC/PR plotting.\")\n    else:\n        roc_handles = []\n        pr_handles  = []\n        fig_roc, ax_roc = plt.subplots(figsize=(6.5,5))\n        fig_pr,  ax_pr  = plt.subplots(figsize=(6.5,5))\n\n        yte = seq_data[\"test\"][\"y\"].astype(int)\n        Xte = seq_data[\"test\"][\"X_seq\"]\n        Xte_stat = seq_data[\"test\"].get(\"X_stat\", None)\n\n        for _, r in top5_overall.iterrows():\n            meth, arch = r[\"Method\"], r[\"Model\"]\n            ckpt = os.path.join(CHECKPOINT_DIR, f\"{meth}__{arch}.h5\")\n            if not os.path.exists(ckpt):\n                print(f\"⚠️ Checkpoint not found for {meth}/{arch}: {ckpt} — skipping.\")\n                continue\n\n            try:\n                model = tf.keras.models.load_model(ckpt, custom_objects={\"loss\": focal_loss()}, compile=False)\n            except Exception as e:\n                print(f\"⚠️ Failed to load model {ckpt}: {e}\")\n                continue\n\n            y_prob = _predict_loaded_model(model, Xte, X_stat=Xte_stat)\n            if y_prob is None:\n                continue\n\n            # ROC\n            try:\n                fpr, tpr, _ = roc_curve(yte, y_prob)\n                roc_auc = auc(fpr, tpr)\n                ax_roc.plot(fpr, tpr, label=f'{meth}/{arch} (AUC={roc_auc:.3f})')\n            except Exception as e:\n                print(f\"⚠️ ROC failed for {meth}/{arch}: {e}\")\n\n            # PR\n            try:\n                prec, rec, _ = precision_recall_curve(yte, y_prob)\n                ap = average_precision_score(yte, y_prob)\n                ax_pr.plot(rec, prec, label=f'{meth}/{arch} (AP={ap:.3f})')\n            except Exception as e:\n                print(f\"⚠️ PR failed for {meth}/{arch}: {e}\")\n\n        # finalize ROC\n        ax_roc.plot([0,1],[0,1],'--', color='gray', label='Random')\n        ax_roc.set_title(\"ROC — Best 5 by VAL F1\")\n        ax_roc.set_xlabel(\"False Positive Rate\")\n        ax_roc.set_ylabel(\"True Positive Rate\")\n        ax_roc.legend(fontsize=8)\n        plt.tight_layout()\n        roc_png = \"/kaggle/working/best5_roc.png\" if os.path.exists(\"/kaggle/working\") else \"best5_roc.png\"\n        fig_roc.savefig(roc_png, dpi=250); plt.close(fig_roc)\n        print(f\"🖼️ Saved ROC curves → {roc_png}\")\n\n        # finalize PR\n        ax_pr.set_title(\"Precision–Recall — Best 5 by VAL F1\")\n        ax_pr.set_xlabel(\"Recall\")\n        ax_pr.set_ylabel(\"Precision\")\n        ax_pr.legend(fontsize=8)\n        plt.tight_layout()\n        pr_png = \"/kaggle/working/best5_pr.png\" if os.path.exists(\"/kaggle/working\") else \"best5_pr.png\"\n        fig_pr.savefig(pr_png, dpi=250); plt.close(fig_pr)\n        print(f\"🖼️ Saved PR curves → {pr_png}\")\n\n# ---------- 4) Feature counts (hourly vs visit vs static) ----------\nif seq_data is not None:\n    seq_feats = seq_data.get(\"seq_features_used\", []) or []\n    static_feats = seq_data.get(\"static_features_used\", []) or []\n    visit_used = [f for f in seq_feats if f in VISIT_COLS]\n    hourly_used = [f for f in seq_feats if f not in VISIT_COLS]\n\n    print(\"\\n=== Feature sets used (from NPZ) ===\")\n    print(f\"Hourly features after transform: {len(hourly_used)} → {hourly_used}\")\n    print(f\"Static features after transform: {len(static_feats)} → {static_feats}\")\n    print(f\"Visit features: {len(visit_used)} → {visit_used}\")\nelse:\n    print(\"\\n⚠️ Feature counts unavailable (NPZ not found).\")\n\n# ---------- 5) Sequence shapes & class distribution ----------\ndef _shape_or_na(arr):\n    try:\n        return tuple(arr.shape)\n    except Exception:\n        return \"(NA)\"\n\ndef _dist_str(y):\n    if y is None or len(y)==0:\n        return \"{ } (pos=NA)\"\n    cnt = Counter(np.asarray(y).astype(int).tolist())\n    pos = cnt.get(1,0); tot = sum(cnt.values())\n    pct = 100.0*pos/max(1,tot)\n    return f\"{dict(cnt)} (pos={pct:.1f}%)\"\n\nif seq_data is not None:\n    trX, vaX, teX = seq_data[\"train\"][\"X_seq\"], seq_data[\"val\"][\"X_seq\"], seq_data[\"test\"][\"X_seq\"]\n    print(\"\\n=== Sequence shapes ===\")\n    print(f\"Train seq: {_shape_or_na(trX)}\")\n    print(f\"Val   seq: {_shape_or_na(vaX)}\")\n    print(f\"Test  seq: {_shape_or_na(teX)}\")\n\n    print(\"\\n=== Class distribution ===\")\n    print(f\"🔎 Train sequences: {_dist_str(seq_data['train']['y'])}\")\n    print(f\"🔎 Val sequences:   {_dist_str(seq_data['val']['y'])}\")\n    print(f\"🔎 Test sequences:  {_dist_str(seq_data['test']['y'])}\")\nelse:\n    print(\"\\n⚠️ Sequence shapes & class distribution unavailable (NPZ not found).\")\n\n# ---------- 6) Print structure of the BEST model ----------\nif df_val.empty:\n    print(\"\\n⚠️ No validation rows → cannot determine best model for summary.\")\nelse:\n    best_row = df_val.sort_values(\"Overall/F1_weighted\", ascending=False).iloc[0]\n    meth, arch = best_row[\"Method\"], best_row[\"Model\"]\n    ckpt = os.path.join(CHECKPOINT_DIR, f\"{meth}__{arch}.h5\")\n    if not os.path.exists(ckpt):\n        print(f\"\\n⚠️ Best model checkpoint not found: {ckpt}\")\n    else:\n        try:\n            model = tf.keras.models.load_model(ckpt, custom_objects={\"loss\": focal_loss()}, compile=False)\n            s = io.StringIO()\n            model.summary(print_fn=lambda x: s.write(x + \"\\n\"))\n            summary_text = s.getvalue()\n            print(\"\\n=== Best model structure (Keras summary) ===\")\n            print(summary_text)\n            # Save to file\n            summary_path = \"/kaggle/working/best_model_summary.txt\" if os.path.exists(\"/kaggle/working\") else \"best_model_summary.txt\"\n            with open(summary_path, \"w\") as f:\n                f.write(summary_text)\n            print(f\"💾 Saved best model summary → {summary_path}\")\n        except Exception as e:\n            print(f\"\\n⚠️ Failed to load/print model summary: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:59:19.120945Z","iopub.execute_input":"2025-10-18T08:59:19.121502Z","iopub.status.idle":"2025-10-18T08:59:32.588905Z","shell.execute_reply.started":"2025-10-18T08:59:19.121470Z","shell.execute_reply":"2025-10-18T08:59:32.587999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================================\n# Extra: fixed-confusion-matrix PNGs for\n#  - oversample_seq + LSTM_100 @ thr 0.50\n#  - undersample_seq + BiLSTM  @ thr 0.40\n#  - none          + LSTM_50   @ thr 0.50\n# ===========================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ---- where to find the results file ----\nRESULTS_CSV_CANDIDATES = [\n    \"/mnt/data/results_summary_all (9).csv\",\n    \"/mnt/data/results_summary_all.csv\",\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\n\ndef _first_existing(paths):\n    for p in paths:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"Make sure Method/Model/Threshold/Split are present (parse from Key if needed).\"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file missing Method/Model/Threshold/Split and no 'Key' to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"),   \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", np.nan))))\n    return df\n\ndef _confmat_from_row(row: pd.Series):\n    \"\"\"\n    Build confusion matrix from supports + recall/spec recorded in results CSV:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N  (fallback to Class0/Recall if needed)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n\n    sp1 = row.get(\"Class1/Specificity\", np.nan)\n    if not np.isfinite(sp1):\n        # fallback: recall for class 0 equals specificity wrt class 1\n        sp1 = float(row.get(\"Class0/Recall\", np.nan))\n\n    if not all(np.isfinite([s0, s1, rec1, sp1])):\n        raise ValueError(\"Row lacks required metrics to reconstruct confusion matrix.\")\n\n    tp = int(round(rec1 * s1))\n    fn = s1 - tp\n    tn = int(round(sp1 * s0))\n    fp = s0 - tn\n\n    # clamp for safety\n    tn = max(0, min(tn, s0)); fp = s0 - tn\n    tp = max(0, min(tp, s1)); fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef _plot_cm(cm: np.ndarray, title: str, out_path: str):\n    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v),\n                ha=\"center\", va=\"center\",\n                color=(\"white\" if v > cm.max()/2 else \"black\"),\n                fontsize=12)\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=220)\n    plt.close(fig)\n\n# ---- load results ----\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results_summary_all*.csv — update RESULTS_CSV_CANDIDATES.\")\nprint(f\"📄 Using results file: {res_path}\")\nres = pd.read_csv(res_path)\nres = _ensure_columns(res)\n\n# We'll take the 'test' split by default (change SPLIT to \"val\" if you want VAL instead)\nSPLIT = \"test\"\n\n# ---- combos you requested ----\nrequests = [\n    (\"oversample_seq\", \"LSTM_100\", 0.50),\n    (\"undersample_seq\", \"BiLSTM\",  0.40),\n    (\"none\",           \"LSTM_50\",  0.50),\n]\n\n# ---- make outputs dir if on Kaggle ----\nout_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\noutputs = []\n\nfor method, model, thr in requests:\n    # filter rows (case-insensitive Split; Threshold rounded to 2dp in pipeline)\n    sub = res[\n        (res[\"Method\"] == method) &\n        (res[\"Model\"]  == model)  &\n        (res[\"Split\"].astype(str).str.lower() == SPLIT.lower())\n    ].copy()\n\n    if sub.empty:\n        print(f\"⚠️ No rows for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # match threshold to 2 decimals\n    # accept both exact and near-equal due to float rounding\n    def _thr_match(x):\n        try:\n            return (round(float(x), 2) == round(float(thr), 2)) or (abs(float(x) - float(thr)) < 1e-6)\n        except Exception:\n            return False\n\n    sub = sub[sub[\"Threshold\"].apply(_thr_match)]\n    if sub.empty:\n        print(f\"⚠️ No threshold={thr:.2f} row for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # If multiple rows (rare), take the first\n    row = sub.iloc[0]\n    cm = _confmat_from_row(row)\n\n    # Print nicely\n    print(f\"\\n=== Confusion Matrix — {method} + {model} @ thr={thr:.2f} [{SPLIT}] ===\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Save PNG\n    safe_method = method.replace(\"/\",\"-\")\n    safe_model  = model.replace(\"/\",\"-\")\n    out_png = os.path.join(out_dir, f\"cm_{safe_method}__{safe_model}__thr_{thr:.2f}__{SPLIT}.png\")\n    _plot_cm(cm, f\"{method} + {model} @ thr={thr:.2f} [{SPLIT}]\", out_png)\n    outputs.append(out_png)\n\nprint(\"\\n🖼️ Saved confusion matrix images:\")\nfor p in outputs:\n    print(\" -\", p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T09:43:08.635081Z","iopub.execute_input":"2025-10-18T09:43:08.635547Z","iopub.status.idle":"2025-10-18T09:43:08.666415Z","shell.execute_reply.started":"2025-10-18T09:43:08.635524Z","shell.execute_reply":"2025-10-18T09:43:08.665256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  fixed-confusion-matrix PNGs","metadata":{}},{"cell_type":"code","source":"# ===========================================\n# Extra: fixed-confusion-matrix PNGs for\n#  - oversample_seq + LSTM_100 @ thr 0.50\n#  - undersample_seq + BiLSTM  @ thr 0.40\n#  - none          + LSTM_50   @ thr 0.50\n# ===========================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ---- where to find the results file ----\nRESULTS_CSV_CANDIDATES = [\n    \"/mnt/data/results_summary_all (9).csv\",\n    \"/mnt/data/results_summary_all.csv\",\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\n\ndef _first_existing(paths):\n    for p in paths:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"Make sure Method/Model/Threshold/Split are present (parse from Key if needed).\"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file missing Method/Model/Threshold/Split and no 'Key' to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"),   \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", np.nan))))\n    return df\n\ndef _confmat_from_row(row: pd.Series):\n    \"\"\"\n    Build confusion matrix from supports + recall/spec recorded in results CSV:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N  (fallback to Class0/Recall if needed)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n\n    sp1 = row.get(\"Class1/Specificity\", np.nan)\n    if not np.isfinite(sp1):\n        # fallback: recall for class 0 equals specificity wrt class 1\n        sp1 = float(row.get(\"Class0/Recall\", np.nan))\n\n    if not all(np.isfinite([s0, s1, rec1, sp1])):\n        raise ValueError(\"Row lacks required metrics to reconstruct confusion matrix.\")\n\n    tp = int(round(rec1 * s1))\n    fn = s1 - tp\n    tn = int(round(sp1 * s0))\n    fp = s0 - tn\n\n    # clamp for safety\n    tn = max(0, min(tn, s0)); fp = s0 - tn\n    tp = max(0, min(tp, s1)); fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef _plot_cm(cm: np.ndarray, title: str, out_path: str):\n    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v),\n                ha=\"center\", va=\"center\",\n                color=(\"white\" if v > cm.max()/2 else \"black\"),\n                fontsize=12)\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=220)\n    plt.close(fig)\n\n# ---- load results ----\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results_summary_all*.csv — update RESULTS_CSV_CANDIDATES.\")\nprint(f\"📄 Using results file: {res_path}\")\nres = pd.read_csv(res_path)\nres = _ensure_columns(res)\n\n# We'll take the 'test' split by default (change SPLIT to \"val\" if you want VAL instead)\nSPLIT = \"test\"\n\n# ---- combos you requested ----\nrequests = [\n    (\"oversample_seq\", \"LSTM_100\", 0.50),\n    (\"undersample_seq\", \"BiLSTM\",  0.40),\n    (\"none\",           \"LSTM_50\",  0.50),\n]\n\n# ---- make outputs dir if on Kaggle ----\nout_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\noutputs = []\n\nfor method, model, thr in requests:\n    # filter rows (case-insensitive Split; Threshold rounded to 2dp in pipeline)\n    sub = res[\n        (res[\"Method\"] == method) &\n        (res[\"Model\"]  == model)  &\n        (res[\"Split\"].astype(str).str.lower() == SPLIT.lower())\n    ].copy()\n\n    if sub.empty:\n        print(f\"⚠️ No rows for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # match threshold to 2 decimals\n    # accept both exact and near-equal due to float rounding\n    def _thr_match(x):\n        try:\n            return (round(float(x), 2) == round(float(thr), 2)) or (abs(float(x) - float(thr)) < 1e-6)\n        except Exception:\n            return False\n\n    sub = sub[sub[\"Threshold\"].apply(_thr_match)]\n    if sub.empty:\n        print(f\"⚠️ No threshold={thr:.2f} row for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # If multiple rows (rare), take the first\n    row = sub.iloc[0]\n    cm = _confmat_from_row(row)\n\n    # Print nicely\n    print(f\"\\n=== Confusion Matrix — {method} + {model} @ thr={thr:.2f} [{SPLIT}] ===\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Save PNG\n    safe_method = method.replace(\"/\",\"-\")\n    safe_model  = model.replace(\"/\",\"-\")\n    out_png = os.path.join(out_dir, f\"cm_{safe_method}__{safe_model}__thr_{thr:.2f}__{SPLIT}.png\")\n    _plot_cm(cm, f\"{method} + {model} @ thr={thr:.2f} [{SPLIT}]\", out_png)\n    outputs.append(out_png)\n\nprint(\"\\n🖼️ Saved confusion matrix images:\")\nfor p in outputs:\n    print(\" -\", p)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Shape \n### Fixed analysis + sequence utilities (single-source, de-duplicated)\n### - build_sequences_by_split   (scales using actual X.shape, not SEQ_LEN)\n### - _build_sequence_index_map  (one canonical copy)\n### - compute_visit_shap         (robust, seq_len inferred if omitted)\n### - aggregate_hourly_to_daily_risk (no hidden deps; direct model.predict)\n### - make_balanced_test         (shape-safe)","metadata":{}},{"cell_type":"code","source":"# ======================================================\n# Fixed analysis + sequence utilities + RUNNER\n#  - build_sequences_by_split   (scales using actual X.shape, not SEQ_LEN)\n#  - _build_sequence_index_map  (one canonical copy)\n#  - compute_visit_shap         (robust; seq_len inferred if omitted)\n#  - aggregate_hourly_to_daily_risk (direct model.predict; no hidden deps)\n#  - make_balanced_test         (shape-safe)\n#  - run_analyses_all           (calls everything and prints outputs)\n#  - quick model fit if none    (small LSTM; early stopping)\n# ======================================================\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Optional TF imports only used in the quick-fit path\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# ---------------------------\n# Global guards / safe fallbacks\n# ---------------------------\ndef _bool_env(name, default=False):\n    v = os.environ.get(name)\n    if v is None: return default\n    return str(v).strip().lower() in {\"1\",\"true\",\"yes\",\"y\",\"on\"}\n\ntry:\n    USE_STATIC_INPUT\nexcept NameError:\n    USE_STATIC_INPUT = True\n\ntry:\n    RANDOM_STATE\nexcept NameError:\n    RANDOM_STATE = 42\n\ntry:\n    DEFAULT_SEQ_FEATURE_COLS\nexcept NameError:\n    DEFAULT_SEQ_FEATURE_COLS = (\n        \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n        \"pc1_activity_energy\",\n        \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n    )\n\ntry:\n    STATIC_COLS\nexcept NameError:\n    STATIC_COLS = [\n        \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n        \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n    ]\n\ntry:\n    VISIT_COLS\nexcept NameError:\n    VISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\n\n# Analysis defaults\nVISIT_FEATURES       = VISIT_COLS\nSHAP_BACKGROUND_SIZE = 512\nSHAP_TEST_SAMPLES    = 1024\nRISK_THRESHOLD       = 0.50\n\ndef _check_globals():\n    \"\"\"No-op guard used by helpers that previously referenced external globals.\"\"\"\n    pass\n\n# ======================================================\n# build_sequences_by_split (uses actual X.shape[1] when reshaping)\n# ======================================================\ndef build_sequences_by_split(\n    hourly: pd.DataFrame,\n    splits,\n    seq_len: int,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features: bool = True\n):\n    \"\"\"\n    Build (X_seq, X_stat, y) arrays for train/val/test given hourly data and patient splits.\n    - Scalers are fit on TRAIN only.\n    - Uses actual window length (T) from arrays when reshaping; no reliance on global SEQ_LEN.\n    \"\"\"\n    # Required columns\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    # Sequence features presence\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    # Static matrix per patient (fill zeros for missing patients)\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float)\n                      .fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        sub = sub.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len])\n                y.append(labels[i+seq_len])\n                if USE_STATIC_INPUT and static_mat is not None:\n                    if pid in static_mat.index:\n                        X_stat.append(static_mat.loc[pid].values.astype(float))\n                    else:\n                        X_stat.append(np.zeros(len(static_cols_present), dtype=float))\n        X_seq = np.array(X_seq)\n        y     = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (USE_STATIC_INPUT and static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    # -------- Scale on TRAIN only (use actual T, F) --------\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s is not None and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size == 0:\n                return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size > 0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size == 0:\n                return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"✅ Sequences built | train={getattr(Xtr_s,'shape',None)}, val={getattr(Xva_s,'shape',None)}, test={getattr(Xte_s,'shape',None)}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ======================================================\n# Sequence index map (single canonical copy)\n# ======================================================\ndef _build_sequence_index_map(hourly_df: pd.DataFrame, split: str, seq_len: int) -> pd.DataFrame:\n    \"\"\"\n    Recreate the exact sequence ordering used in build_sequences_by_split so that\n    index i maps to the (i+seq_len)-th hour row for each patient.\n\n    Returns: ['seq_idx','patientID','hour','date','visit_assigned','period_main','row_idx']\n    \"\"\"\n    sub = (hourly_df[hourly_df[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"])\n           .reset_index())\n    sub = sub.rename(columns={\"index\": \"row_idx\"})\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt.get(\"date\", pd.NaT)),\n                \"visit_assigned\": tgt.get(\"visit_assigned\", np.nan),\n                \"period_main\": tgt.get(\"period_main\", np.nan),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n# ======================================================\n# SHAP on visit features (robust; seq_len inference; shape checks)\n# ======================================================\ndef compute_visit_shap(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    seq_features_used,\n    visit_features=None,\n    split: str = \"test\",\n    background_size: int = SHAP_BACKGROUND_SIZE,\n    max_test_windows: int = SHAP_TEST_SAMPLES,\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    \"\"\"\n    Computes global and per-visit SHAP importance for visit features that are INCLUDED\n    in the sequence feature tensor. Saves two CSVs:\n      - shap_visit_global.csv\n      - shap_visit_per_visit.csv\n    \"\"\"\n    _check_globals()\n    os.makedirs(out_dir, exist_ok=True)\n    visit_features = list(visit_features) if visit_features is not None else list(VISIT_FEATURES)\n\n    # Infer seq_len from arrays if not provided\n    Xte = data[split][\"X_seq\"]\n    if seq_len is None:\n        if Xte is None or Xte.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xte.shape[1]\n\n    # Map visit features -> indices in the sequence features\n    feat_to_idx = {f: i for i, f in enumerate(seq_features_used)}\n    visit_in_seq = [f for f in visit_features if f in feat_to_idx]\n    if not visit_in_seq:\n        raise ValueError(\n            f\"None of the visit features are present in seq_features_used={seq_features_used}. \"\n            f\"Ensure your DEFAULT_SEQ_FEATURE_COLS includes items from VISIT_COLS.\"\n        )\n\n    Xtr, Xtr_stat = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"]\n    Xte, Xte_stat = data[split][\"X_seq\"],  data[split][\"X_stat\"]\n\n    bg_n = min(int(background_size), len(Xtr))\n    te_n = min(int(max_test_windows), len(Xte))\n    if bg_n < 1 or te_n < 1:\n        raise RuntimeError(f\"Not enough sequences for SHAP (bg_n={bg_n}, te_n={te_n}).\")\n\n    rng   = np.random.default_rng(42)\n    bg_ix = rng.choice(np.arange(len(Xtr)), size=bg_n, replace=False)\n    te_ix = rng.choice(np.arange(len(Xte)), size=te_n, replace=False)\n\n    bg_seq, te_seq = Xtr[bg_ix], Xte[te_ix]\n    if USE_STATIC_INPUT and (Xtr_stat is not None and Xte_stat is not None and Xtr_stat.size>0 and Xte_stat.size>0):\n        bg_static, te_static = Xtr_stat[bg_ix], Xte_stat[te_ix]\n    else:\n        bg_static = te_static = None\n\n    # ---- SHAP explainer\n    try:\n        import shap\n    except Exception as e:\n        raise ImportError(\n            \"This SHAP analysis requires the 'shap' package. Install with: pip install shap\"\n        ) from e\n\n    try:\n        if bg_static is not None:\n            explainer   = shap.DeepExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.DeepExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n    except Exception as e:\n        print(f\"[WARN] DeepExplainer failed ({e}). Falling back to GradientExplainer…\")\n        if bg_static is not None:\n            explainer   = shap.GradientExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.GradientExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n\n    shap_seq = shap_values[0] if isinstance(shap_values, list) else shap_values\n    if shap_seq.ndim != 3:\n        raise RuntimeError(f\"Unexpected SHAP shape for sequence input: {shap_seq.shape}\")\n\n    # Reduce over time → mean |SHAP| across the window\n    shap_abs_time = np.mean(np.abs(shap_seq), axis=1)  # [n_samples, F]\n\n    # ---- GLOBAL visit feature importance\n    rows = [{\"feature\": f, \"mean_abs_shap\": float(np.mean(shap_abs_time[:, feat_to_idx[f]]))}\n            for f in visit_in_seq]\n    global_visit_df = pd.DataFrame(rows).sort_values(\"mean_abs_shap\", ascending=False)\n    gpath = os.path.join(out_dir, \"shap_visit_global.csv\")\n    global_visit_df.to_csv(gpath, index=False)\n    print(\"✅ Saved global visit SHAP →\", gpath)\n\n    # ---- PER‑VISIT importance\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(Xte):\n        raise RuntimeError(\n            f\"Mapping length {len(seq_map)} != X_{split} length {len(Xte)}. \"\n            f\"seq_len={seq_len}. Rebuild hourly/sequences consistently.\"\n        )\n    seq_map_sub = seq_map.iloc[te_ix].reset_index(drop=True)\n\n    per_rows = []\n    for i in range(len(seq_map_sub)):\n        pid = seq_map_sub.loc[i, \"patientID\"]\n        dte = pd.to_datetime(seq_map_sub.loc[i, \"date\"])\n        for f in visit_in_seq:\n            per_rows.append({\n                \"patientID\": pid,\n                \"date\": dte,\n                \"feature\": f,\n                \"mean_abs_shap\": float(shap_abs_time[i, feat_to_idx[f]])\n            })\n    per_visit_df = (pd.DataFrame(per_rows)\n                    .groupby([\"patientID\",\"date\",\"feature\"], as_index=False)[\"mean_abs_shap\"].mean()\n                    .sort_values([\"patientID\",\"date\",\"mean_abs_shap\"], ascending=[True, True, False]))\n    ppath = os.path.join(out_dir, \"shap_visit_per_visit.csv\")\n    per_visit_df.to_csv(ppath, index=False)\n    print(\"✅ Saved per‑visit SHAP →\", ppath)\n\n    return global_visit_df, per_visit_df\n\n# ======================================================\n# Aggregate hourly predictions to daily risk\n# ======================================================\ndef aggregate_hourly_to_daily_risk(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    split: str = \"test\",\n    threshold: float = RISK_THRESHOLD,\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    \"\"\"\n    Aggregate sequence‑window predictions to daily risk summaries.\n    Saves 'daily_risk_<split>.csv' and (best effort) a small example plot.\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    # Predictions\n    Xs      = data[split][\"X_seq\"]\n    Xs_stat = data[split][\"X_stat\"]\n    y_true  = np.asarray(data[split][\"y\"]).astype(int).ravel()\n\n    if seq_len is None:\n        if Xs is None or Xs.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xs.shape[1]\n\n    if USE_STATIC_INPUT and (Xs_stat is not None and Xs_stat.size>0):\n        y_prob = model.predict([Xs, Xs_stat], verbose=0).ravel()\n    else:\n        y_prob = model.predict(Xs, verbose=0).ravel()\n    y_pred = (y_prob >= float(threshold)).astype(int)\n\n    # Map sequence rows back to hours/dates\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(y_true):\n        raise RuntimeError(\n            f\"Sequence map length {len(seq_map)} != prediction length {len(y_true)}.\\n\"\n            f\"seq_len={seq_len}, X_{split}.shape={getattr(Xs, 'shape', None)}. \"\n            \"Use the same hourly/sequences and seq_len that were used for training.\"\n        )\n\n    pred_df = pd.DataFrame({\n        \"patientID\": seq_map[\"patientID\"].values,\n        \"hour\":      pd.to_datetime(seq_map[\"hour\"].values),\n        \"date\":      pd.to_datetime(seq_map[\"date\"].values),\n        \"visit_assigned\": seq_map.get(\"visit_assigned\", pd.Series([np.nan]*len(seq_map))).values,\n        \"period_main\":    seq_map.get(\"period_main\", pd.Series([np.nan]*len(seq_map))).values,\n        \"y_true\":    y_true,\n        \"y_prob\":    y_prob,\n        \"y_pred\":    y_pred\n    })\n\n    # Daily aggregates\n    grp = pred_df.groupby([\"patientID\",\"date\"], as_index=False)\n    daily = grp.agg(\n        n_windows=(\"y_true\",\"size\"),\n        true_positives=(\"y_true\",\"sum\"),\n        pred_positives=(\"y_pred\",\"sum\"),\n        risk_mean=(\"y_prob\",\"mean\"),\n        risk_max=(\"y_prob\",\"max\"),\n        risk_p95=(\"y_prob\", lambda x: float(np.quantile(x, 0.95))),\n        hours_above_thr=(\"y_pred\",\"sum\")\n    )\n    daily[\"prevalence\"] = daily[\"true_positives\"] / daily[\"n_windows\"].replace(0, np.nan)\n    daily_csv = os.path.join(out_dir, f\"daily_risk_{split}.csv\")\n    daily.to_csv(daily_csv, index=False)\n    print(f\"✅ Saved daily risk aggregates → {daily_csv}\")\n\n    # Optional quick plot\n    try:\n        example_pid = daily[\"patientID\"].iloc[0]\n        dsub = daily[daily[\"patientID\"] == example_pid].sort_values(\"date\")\n        plt.figure(figsize=(8,3))\n        plt.plot(dsub[\"date\"], dsub[\"risk_mean\"], label=\"Mean daily risk\")\n        plt.plot(dsub[\"date\"], dsub[\"risk_max\"],  label=\"Max daily risk\")\n        plt.axhline(threshold, linestyle=\"--\", label=f\"thr={threshold:.2f}\")\n        plt.xlabel(\"Date\"); plt.ylabel(\"Risk\"); plt.title(f\"Daily risk — patient {example_pid}\")\n        plt.legend(); plt.tight_layout()\n        png = os.path.join(out_dir, f\"daily_risk_trend_patient_{example_pid}.png\")\n        plt.savefig(png, dpi=200); plt.close()\n        print(f\"🖼️ Saved example daily trend → {png}\")\n    except Exception as e:\n        print(f\"[WARN] Could not plot daily trend example: {e}\")\n\n    return pred_df, daily\n\n# ======================================================\n# Balanced test subset (shape-safe)\n# ======================================================\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state: int = RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0:\n        return (X_test, y_test, (None if X_stat is None else np.asarray(X_stat)))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ======================================================\n# ---------- Helper: tiny LSTM builder for quick run ----------\n# ======================================================\ndef _make_quick_model(seq_len, n_seq_f, n_stat_f=0, lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = LSTM(64, return_sequences=True)(seq_in)\n    x = Dropout(0.2)(x)\n    x = LSTM(32)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(16, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(16, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(16, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(16, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# ======================================================\n# ---------- Helper: load sequences from NPZ ----------\n# ======================================================\ndef _load_sequences_npz(npz_path):\n    d = np.load(npz_path, allow_pickle=True)\n    def _arr(name):\n        return None if name not in d or d[name].size==0 else d[name]\n    data = {\n        \"train\": {\"X_seq\": _arr(\"Xtr\"), \"X_stat\": _arr(\"Xtr_stat\"), \"y\": d[\"ytr\"]},\n        \"val\":   {\"X_seq\": _arr(\"Xva\"), \"X_stat\": _arr(\"Xva_stat\"), \"y\": d[\"yva\"]},\n        \"test\":  {\"X_seq\": _arr(\"Xte\"), \"X_stat\": _arr(\"Xte_stat\"), \"y\": d[\"yte\"]},\n        \"seq_features_used\": list(d[\"seq_features_used\"]),\n        \"static_features_used\": list(d[\"static_features_used\"]) if \"static_features_used\" in d else []\n    }\n    return data\n\n# ======================================================\n# ---------- Helper: find files & build splits ----------\n# ======================================================\ndef _first_existing(paths):\n    for p in paths:\n        if p and os.path.exists(p):\n            return p\n    return None\n\ndef _splits_from_hourly(hourly: pd.DataFrame):\n    train_p = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"train\",\"patientID\"].unique()))\n    val_p   = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"val\",\"patientID\"].unique()))\n    test_p  = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"test\",\"patientID\"].unique()))\n    if len(train_p)==0 or len(val_p)==0 or len(test_p)==0:\n        raise RuntimeError(\"hourly must include a 'Split' column with 'train'/'val'/'test' assignments.\")\n    return (train_p, val_p, test_p)\n\n# ======================================================\n# --------------------- RUNNER -------------------------\n# ======================================================\ndef run_analyses_all(model=None, data=None, hourly=None,\n                     split=\"test\", out_dir=None, seq_len_default=24,\n                     do_shap=True, train_epochs=6):\n    \"\"\"\n    If model/data/hourly are not supplied, this runner tries to:\n    - load sequences from NPZ (sequences_leakfree.npz)\n    - load hourly CSV (dynamic_hourly_features_ramadan.csv)\n    - fit a small LSTM quickly\n    Then it computes:\n    - SHAP CSVs (if 'shap' is installed and do_shap=True)\n    - Daily risk CSV + a small PNG\n    \"\"\"\n    # --- output dir\n    if out_dir is None:\n        if os.path.exists(\"/kaggle/working\"): out_dir = \"/kaggle/working\"\n        else:\n            out_dir = os.path.join(\".\", \"outputs\")\n            os.makedirs(out_dir, exist_ok=True)\n\n    # --- locate files if needed\n    if hourly is None:\n        HOURLY_CANDIDATES = [\n            \"/kaggle/working/dynamic_hourly_features_ramadan.csv\",\n            \"/mnt/data/dynamic_hourly_features_ramadan.csv\",\n            \"/kaggle/input/hmc-model-static-variables/dynamic_hourly_features_ramadan.csv\"\n        ]\n        hp = _first_existing(HOURLY_CANDIDATES)\n        if not hp:\n            raise FileNotFoundError(\"Could not find hourly CSV. Please pass 'hourly' DataFrame.\")\n        hourly = pd.read_csv(hp)\n        print(f\"📄 Loaded hourly table: {hp} | shape={hourly.shape}\")\n\n    if data is None:\n        NPZ_CANDIDATES = [\n            \"/kaggle/working/sequences_leakfree.npz\",\n            \"/kaggle/working/outputs/sequences_leakfree.npz\",\n            \"/mnt/data/sequences_leakfree.npz\"\n        ]\n        npz = _first_existing(NPZ_CANDIDATES)\n        if npz:\n            data = _load_sequences_npz(npz)\n            print(f\"📦 Loaded sequences NPZ: {npz}\")\n        else:\n            # rebuild sequences from hourly\n            splits = _splits_from_hourly(hourly)\n            data = build_sequences_by_split(hourly, splits, seq_len=seq_len_default,\n                                            seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n                                            static_cols=STATIC_COLS, scale_features=True)\n\n    # --- fit quick model if none provided\n    if model is None:\n        Xtr, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"y\"]\n        Xva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"y\"]\n        Xtr_stat = data[\"train\"][\"X_stat\"]; Xva_stat = data[\"val\"][\"X_stat\"]\n        seq_len  = Xtr.shape[1]; n_seq_f = Xtr.shape[2]\n        n_stat_f = 0 if (Xtr_stat is None or not USE_STATIC_INPUT) else Xtr_stat.shape[1]\n\n        model = _make_quick_model(seq_len, n_seq_f, n_stat_f=n_stat_f, lr=1e-3)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xtr, Xtr_stat], ytr, validation_data=([Xva, Xva_stat], yva),\n                      epochs=train_epochs, batch_size=64, callbacks=[es], verbose=1)\n        else:\n            model.fit(Xtr, ytr, validation_data=(Xva, yva),\n                      epochs=train_epochs, batch_size=64, callbacks=[es], verbose=1)\n        print(\"✅ Model trained (quick fit).\")\n\n    # --- SHAP (optional)\n    if do_shap:\n        try:\n            _ = compute_visit_shap(model, data, hourly, data[\"seq_features_used\"],\n                                   visit_features=VISIT_FEATURES, split=split, out_dir=out_dir)\n        except ImportError as e:\n            print(f\"⚠️ SHAP not available; skipping. ({e})\")\n        except Exception as e:\n            print(f\"⚠️ SHAP step skipped due to error: {e}\")\n\n    # --- Daily risk aggregation\n    pred_df, daily = aggregate_hourly_to_daily_risk(model, data, hourly,\n                                                    split=split, threshold=RISK_THRESHOLD,\n                                                    out_dir=out_dir)\n    # Print small previews so you \"see output\"\n    with pd.option_context(\"display.width\", 160, \"display.max_columns\", 20):\n        print(\"\\n🔎 Predictions (head):\")\n        print(pred_df.head(8).to_string(index=False))\n        print(\"\\n📊 Daily risk (head):\")\n        print(daily.head(8).to_string(index=False))\n\n    print(\"\\n🎯 Done. Outputs saved under:\", out_dir)\n    return model, data, hourly\n\n# ======================================================\n# Example: run automatically if this cell/file is executed\n# (You can comment this out if you already have model/data/hourly in memory.)\n# ======================================================\nif __name__ == \"__main__\":\n    _ = run_analyses_all(\n        model=None,          # set to your trained model object to skip quick fit\n        data=None,           # set to your 'data' dict to reuse existing arrays\n        hourly=None,         # set to your hourly DataFrame if already loaded\n        split=\"test\",\n        out_dir=None,        # default (/kaggle/working or ./outputs)\n        seq_len_default=24,  # used only if rebuilding sequences from hourly\n        do_shap=True,        # requires 'pip install shap'\n        train_epochs=6       # small quick fit; adjust as you like\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# forecasts\n\nNotes & options\n\nChange patient: set _patient = <your id> before running.\n\nChange K: set K_HOURS to the forecast horizon you want (e.g., 12).\n\nChange threshold: set THRESH if you prefer the VAL‑optimized threshold you saw in your summary.\n\nBetter inputs for multi‑step: replace the “naive persistence” block with proper hour‑ahead forecasts of your sequence features (CGM, lifestyle PCs, visit vars) for stronger multi‑hour accuracy.\n\n\nNotes for stronger multi‑hour forecasts\n\nYou’re currently using naive persistence for the future input features when rolling forward (i.e., you reuse the last observed hour’s feature vector for each next hour). That’s OK to get started, but for better K>6 accuracy consider:\n\nCGM features (cgm_mean, cgm_std, …): replace persistence with a small CGM forecaster (e.g., ARIMA/Prophet/LightGBM) to generate hour‑ahead CGM summaries, then feed those into your LSTM rolling window.\n\nLifestyle PCs (pc1_activity_energy, …): if you have “typical daily patterns”, a daily seasonal baseline (e.g., by hour‑of‑day) can outperform pure persistence.\n\nVisit features (carb, meals, …): these are daily; step them forward from the last known day or incorporate the next known visit day if available.\n\nIf you want, tell me:\n\na different patientID (from the list above),\n\nyour preferred K_HOURS, and\n\nwhether you want a more conservative (fewer alerts) or more sensitive (more alerts) threshold,\n\nand I’ll output a tailored snippet with those values baked in","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Forecasting & time-compare (fixed + improved)\n# ==========================\nimport os, numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib.dates as mdates\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, precision_recall_curve\nimport tensorflow as tf\n\n# --------- Fallbacks for global symbols if not already defined in the notebook ---------\ntry:\n    OUT_RESULTS_CSV\nexcept NameError:\n    OUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\n\ntry:\n    USE_STATIC_INPUT\nexcept NameError:\n    USE_STATIC_INPUT = True  # your training used static inputs\n\n# focal_loss (redefine here so load_model(custom_objects=...) always works)\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\n# --------------------------\n# 🔧 Parameters you can edit\n# --------------------------\nOVERRIDE_PATIENT_ID        = 69    # force this patient (else it auto-suggests)\nK_HOURS                    = 12    # forecast horizon\nAUTO_USE_VAL_OPTIMAL_THR   = True  # use best VAL threshold from your results CSV\nTHRESH_MANUAL              = 0.49  # fallback if the VAL-optimal isn’t found\nFORECAST_METHOD            = \"ema\" # \"ema\" | \"linear\" | \"persistence\"\nEMA_ALPHA                  = 0.6\nLIN_STEPS                  = 6\nPATIENT_SELECTION_STRATEGY = \"positives\"\nTOP_N_PATIENTS_TO_PLOT     = 3\nSAVE_DIR                   = \"/kaggle/working\"\n\n# Improvement toggles\nUSE_HORIZON_THRESHOLDS     = True  # calibrate per-horizon thresholds on VAL (boosts recall at longer horizons)\nFBETA_FOR_CAL              = 1.5   # β>1 favors recall\nAPPLY_NOFM                 = True  # apply N-of-M decision smoothing\nNOFM_N, NOFM_M             = 2, 3  # 2 of last 3 positives -> positive\n\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# --- Datetime helpers: make all times tz-naive in UTC ---\ndef _to_naive_utc(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    # If tz-aware, convert to UTC then drop tz; if tz-naive, leave as is\n    if getattr(s.dt, \"tz\", None) is not None:\n        s = s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n    return s\n\n# ----------------------------------------------------\n# 0) Utilities: choose best checkpoint & VAL threshold\n# ----------------------------------------------------\ndef pick_best_checkpoint(results_csv=OUT_RESULTS_CSV, ckpt_dir=\"checkpoints\"):\n    if not os.path.exists(results_csv):\n        raise FileNotFoundError(f\"Results CSV not found: {results_csv}\")\n    df = pd.read_csv(results_csv)\n    dfv = df[df[\"Split\"].astype(str).str.lower().eq(\"val\")].copy()\n    if dfv.empty:\n        raise RuntimeError(\"No VAL rows found in results; cannot pick best model.\")\n    dfv = dfv.sort_values(\"Overall/F1_weighted\", ascending=False)\n    best = dfv.iloc[0]\n    method = str(best[\"Method\"]); model = str(best[\"Model\"])\n    ckpt_path = os.path.join(ckpt_dir, f\"{method}__{model}.h5\")\n    if not os.path.exists(ckpt_path):\n        files = [os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.endswith(\".h5\")]\n        if not files: raise FileNotFoundError(\"No .h5 checkpoints found in 'checkpoints'.\")\n        ckpt_path = files[0]\n        print(f\"[WARN] Expected checkpoint not found; using {ckpt_path}\")\n    print(f\"✅ Best (VAL F1) → {method}/{model} | ckpt = {ckpt_path}\")\n    return ckpt_path, method, model\n\ndef get_val_optimal_threshold(results_csv, method, model):\n    try:\n        df = pd.read_csv(results_csv)\n        dfv = df[(df[\"Split\"].astype(str).str.lower()==\"val\") &\n                 (df[\"Method\"].astype(str)==method) &\n                 (df[\"Model\"].astype(str)==model)].copy()\n        if dfv.empty:\n            dfv = df[df[\"Split\"].astype(str).str.lower()==\"val\"].copy()\n        dfv = dfv.sort_values(\"Overall/F1_weighted\", ascending=False)\n        t = float(dfv.iloc[0][\"Threshold\"])\n        if np.isfinite(t): return t\n    except Exception as e:\n        print(f\"[WARN] Could not read VAL-optimal threshold: {e}\")\n    return None\n\n# ------------------------------------------------\n# 1) Align predictions to hours for a given split\n# ------------------------------------------------\ndef predict_split_prob_df(model, data, hourly, split=\"test\", threshold=0.50):\n    Xs = data[split][\"X_seq\"]; Xstat = data[split][\"X_stat\"]\n    ytrue = data[split][\"y\"].astype(int).ravel()\n    if Xs is None or Xs.ndim != 3:\n        raise ValueError(f\"No sequences for split={split}.\")\n    yprob = model.predict([Xs, Xstat], verbose=0).ravel() if (Xstat is not None and Xstat.size>0) else model.predict(Xs, verbose=0).ravel()\n    ypred = (yprob >= float(threshold)).astype(int)\n    seq_len = Xs.shape[1]\n\n    sub = (hourly[hourly[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"]).reset_index())\n    sub = sub.rename(columns={\"index\":\"row_idx\"})\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\"seq_idx\":len(rows), \"patientID\":pid,\n                         \"hour\":pd.to_datetime(tgt[\"hour\"]),\n                         \"date\":pd.to_datetime(tgt.get(\"date\", pd.NaT))})\n    idx_map = pd.DataFrame(rows)\n    if len(idx_map) != len(ytrue):\n        raise RuntimeError(f\"Mapping length {len(idx_map)} != predictions length {len(ytrue)}.\")\n    out = (pd.DataFrame({\n                \"patientID\": idx_map[\"patientID\"].values,\n                \"hour\":      pd.to_datetime(idx_map[\"hour\"].values),\n                \"date\":      pd.to_datetime(idx_map[\"date\"].values),\n                \"y_true\":    ytrue,\n                \"y_prob\":    yprob,\n                \"y_pred\":    ypred\n            })\n            .sort_values([\"patientID\",\"hour\"]).reset_index(drop=True))\n    out[\"hour\"] = _to_naive_utc(out[\"hour\"])  # belt-and-suspenders\n    return out\n\n# ------------------------------------------------------------\n# 2) Mini feature forecaster for multi-step rolling predictions\n# ------------------------------------------------------------\ndef _ema_next(v, alpha=0.6):\n    s = v[0]\n    for x in v[1:]:\n        s = alpha*x + (1-alpha)*s\n    return float(s)\n\ndef _linear_next(v):\n    n = len(v)\n    if n < 2: return float(v[-1])\n    x = np.arange(n, dtype=float)\n    try:\n        b1, b0 = np.polyfit(x, v.astype(float), 1)  # y = b1*x + b0\n        return float(b1*(n) + b0)\n    except Exception:\n        return float(v[-1])\n\ndef next_feature_vector(hist_raw, feat_names, method=\"ema\", ema_alpha=0.6, lin_steps=6):\n    T, F = hist_raw.shape\n    out = np.zeros(F, dtype=float)\n    for j, name in enumerate(feat_names):\n        col = hist_raw[:, j]\n        if method == \"persistence\":\n            out[j] = float(col[-1])\n        elif method == \"linear\":\n            w = min(len(col), max(2, lin_steps))\n            out[j] = _linear_next(col[-w:])\n        else:  # \"ema\" default for dynamic signals\n            if any(k in str(name).lower() for k in [\"cgm\",\"pca\",\"pc1\",\"pc2\",\"pc3\",\"steps\",\"calories\",\"heart\"]):\n                w = min(len(col), max(2, lin_steps))\n                out[j] = _ema_next(col[-w:], alpha=ema_alpha)\n            else:\n                out[j] = float(col[-1])\n    return out\n\n# -----------------------------------------------------------------\n# 3) Prepare one anchor window & rolling multi-step patient forecast\n# -----------------------------------------------------------------\ndef _prepare_window_for_patient_index(hourly, data, patient_id, idx, split=\"test\"):\n    seq_feats = list(data[\"seq_features_used\"])\n    seq_len   = int(data[\"train\"][\"X_seq\"].shape[1])\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    if idx < seq_len: raise ValueError(\"idx must be >= seq_len\")\n    hist_raw   = sub.loc[idx-seq_len:idx-1, seq_feats].astype(float).values\n    seq_scaler = data[\"scalers\"][\"seq\"]\n    hist_scaled= seq_scaler.transform(hist_raw) if seq_scaler is not None else hist_raw\n    # static\n    if USE_STATIC_INPUT and data[\"train\"][\"X_stat\"] is not None:\n        stat_feats = list(data.get(\"static_features_used\", []))\n        srow = (hourly[[\"patientID\"]+stat_feats].drop_duplicates(subset=[\"patientID\"]).set_index(\"patientID\"))\n        s = (srow.loc[patient_id].astype(float).values if patient_id in srow.index else np.zeros(len(stat_feats), dtype=float))\n        stat_scaler = data[\"scalers\"][\"stat\"]\n        if stat_scaler is not None and s.size>0:\n            s = stat_scaler.transform(s.reshape(1,-1)).ravel()\n    else:\n        s = None\n    last_hour  = pd.to_datetime(sub.loc[idx-1, \"hour\"])\n    return hist_scaled, s, hist_raw, last_hour, sub\n\ndef rolling_forecast_patient(model, data, hourly, patient_id, k=6, split=\"test\", threshold=0.50,\n                             method=FORECAST_METHOD, ema_alpha=EMA_ALPHA, lin_steps=LIN_STEPS):\n    seq_feats   = list(data[\"seq_features_used\"])\n    seq_len     = int(data[\"train\"][\"X_seq\"].shape[1])\n    seq_scaler  = data[\"scalers\"][\"seq\"]\n\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    rows = []\n    for anchor_idx in range(seq_len, len(sub)-1):  # each anchor predicts +1..+k\n        last_needed = anchor_idx + k\n        if last_needed >= len(sub): break\n        Xwin_scaled, svec, raw_hist, _, _ = _prepare_window_for_patient_index(hourly, data, patient_id, anchor_idx, split=split)\n        cur_scaled = Xwin_scaled.copy()\n        cur_raw    = raw_hist.copy()\n        F = cur_scaled.shape[1]\n        for h in range(1, k+1):\n            xin = cur_scaled.reshape(1, seq_len, F)\n            prob = (model.predict([xin, svec.reshape(1,-1)], verbose=0).ravel()[0]\n                    if (svec is not None and USE_STATIC_INPUT) else\n                    model.predict(xin, verbose=0).ravel()[0])\n            tgt_idx  = anchor_idx + h\n            tgt_hour = pd.to_datetime(sub.loc[tgt_idx, \"hour\"])\n            y_true   = int(sub.loc[tgt_idx, \"hypo_label\"])\n            # horizon-specific threshold support\n            thr_h = (threshold.get(h, float(threshold.get(1, 0.50)))\n                     if isinstance(threshold, dict) else float(threshold))\n            rows.append({\n                \"patientID\": patient_id,\n                \"anchor_hour\": pd.to_datetime(sub.loc[anchor_idx, \"hour\"]),\n                \"forecast_hour\": tgt_hour,\n                \"horizon\": h,\n                \"y_prob\": float(prob),\n                \"y_pred\": int(prob >= thr_h),\n                \"y_true\": y_true\n            })\n            # roll features using mini forecaster\n            next_raw    = next_feature_vector(cur_raw, seq_feats, method=method, ema_alpha=ema_alpha, lin_steps=lin_steps)\n            next_scaled = seq_scaler.transform(next_raw.reshape(1,-1)).ravel() if seq_scaler is not None else next_raw\n            cur_scaled  = np.vstack([cur_scaled[1:], next_scaled])\n            cur_raw     = np.vstack([cur_raw[1:], next_raw])\n    out = (pd.DataFrame(rows).sort_values([\"forecast_hour\",\"horizon\"]).reset_index(drop=True))\n    out[\"forecast_hour\"] = _to_naive_utc(out[\"forecast_hour\"])  # <<< tz-fix\n    out[\"anchor_hour\"]   = _to_naive_utc(out[\"anchor_hour\"])\n    return out\n\n# -----------------------------------------\n# 4) Metrics by horizon + patient suggestion\n# -----------------------------------------\ndef metrics_by_horizon(df_forecast):\n    out = []\n    for h, g in df_forecast.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        yhat = g[\"y_pred\"].astype(int).values\n        try: roc = roc_auc_score(y, p)\n        except: roc = np.nan\n        try: pr  = average_precision_score(y, p)\n        except: pr  = np.nan\n        out.append({\n            \"horizon\": h, \"n\": len(g),\n            \"Accuracy\": accuracy_score(y, yhat),\n            \"Precision\": precision_score(y, yhat, zero_division=0),\n            \"Recall\": recall_score(y, yhat, zero_division=0),\n            \"F1\": f1_score(y, yhat, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr\n        })\n    return pd.DataFrame(out).sort_values(\"horizon\")\n\ndef metrics_by_horizon_on_col(df_forecast, col=\"y_pred\"):\n    rows = []\n    for h, g in df_forecast.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        yhat = g[col].astype(int).values\n        try: roc = roc_auc_score(y, g[\"y_prob\"].astype(float).values)\n        except: roc = np.nan\n        try: pr  = average_precision_score(y, g[\"y_prob\"].astype(float).values)\n        except: pr  = np.nan\n        rows.append({\n            \"horizon\": h, \"n\": len(g),\n            \"Accuracy\": accuracy_score(y, yhat),\n            \"Precision\": precision_score(y, yhat, zero_division=0),\n            \"Recall\": recall_score(y, yhat, zero_division=0),\n            \"F1\": f1_score(y, yhat, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr,\n            \"decision\": col\n        })\n    return pd.DataFrame(rows).sort_values(\"horizon\")\n\ndef suggest_patients_for_plots(df_nowcast, strategy=\"positives\", top_n=3):\n    g = df_nowcast.groupby(\"patientID\", as_index=False)\n    if strategy == \"coverage\":\n        s = g.size().rename(columns={\"size\":\"n\"}).sort_values(\"n\", ascending=False)\n        ids = list(s[\"patientID\"].head(top_n))\n    elif strategy == \"auc\":\n        rows = []\n        for pid, grp in df_nowcast.groupby(\"patientID\"):\n            y, p = grp[\"y_true\"].values, grp[\"y_prob\"].values\n            try: pr = average_precision_score(y, p)\n            except: pr = np.nan\n            rows.append({\"patientID\":pid, \"PR_AUC\":pr, \"n\":len(grp)})\n        s = (pd.DataFrame(rows).dropna(subset=[\"PR_AUC\"])\n             .sort_values([\"PR_AUC\",\"n\"], ascending=[False, False]))\n        ids = list(s[\"patientID\"].head(top_n)) if not s.empty else list(df_nowcast[\"patientID\"].value_counts().head(top_n).index)\n    else:  # \"positives\" default\n        pos = g[\"y_true\"].sum().rename(columns={\"y_true\":\"positives\"}) \\\n               .sort_values(\"positives\", ascending=False)\n        ids = list(pos[\"patientID\"].head(top_n))\n        if pos[\"positives\"].max() <= 0:\n            ids = list(df_nowcast[\"patientID\"].value_counts().head(top_n).index)\n    return ids\n\n# -----------------------\n# 5) Post-processing (N-of-M)\n# -----------------------\ndef apply_n_of_m_rule(df, n=2, m=3):\n    \"\"\"df must be one patient; columns: forecast_hour, y_pred.\"\"\"\n    df = df.sort_values(\"forecast_hour\").copy()\n    flags = df[\"y_pred\"].astype(int).values\n    out = np.zeros_like(flags)\n    for i in range(len(flags)):\n        win = flags[max(0, i-m+1):i+1]\n        out[i] = 1 if win.sum() >= n else 0\n    df[\"y_pred_nofm\"] = out\n    return df\n\n# -----------------------\n# 6) Plotting functions\n# -----------------------\ndef plot_nowcast_and_forecast_timeline(df_nowcast, df_forecast, patient_id,\n                                       hours_back=72, out_png=None, threshold=0.50):\n    past = df_nowcast[df_nowcast[\"patientID\"]==patient_id].sort_values(\"hour\").copy()\n    past[\"hour\"] = _to_naive_utc(past[\"hour\"])  # <<< tz-fix\n    if not past.empty and hours_back and hours_back>0:\n        cutoff = past[\"hour\"].max() - pd.Timedelta(hours=hours_back)\n        past = past[past[\"hour\"] >= cutoff]\n    if df_forecast.empty: raise ValueError(\"df_forecast is empty.\")\n    last_anchor = df_forecast[df_forecast[\"patientID\"]==patient_id][\"anchor_hour\"].max()\n    fut = (df_forecast[(df_forecast[\"patientID\"]==patient_id) &\n                       (df_forecast[\"anchor_hour\"]==last_anchor)]\n           .sort_values(\"forecast_hour\").copy())\n    fut[\"forecast_hour\"] = _to_naive_utc(fut[\"forecast_hour\"])  # <<< tz-fix\n\n    fig, ax = plt.subplots(figsize=(10,4))\n    if not past.empty:\n        ax.plot(past[\"hour\"], past[\"y_prob\"], lw=2, label=\"Nowcast (test) prob\")\n        ax.step(past[\"hour\"], past[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (past)\")\n    ax.plot(fut[\"forecast_hour\"], fut[\"y_prob\"], lw=2, marker=\"o\",\n            label=f\"Forecast next {int(fut['horizon'].max())}h prob\")\n    if \"y_true\" in fut.columns and fut[\"y_true\"].notna().any():\n        ax.step(fut[\"forecast_hour\"], fut[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (future)\")\n    ax.axhline(float(threshold) if not isinstance(threshold, dict) else float(threshold.get(1, 0.50)),\n               ls=\"--\", label=\"Decision threshold\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(\"Hour\"); ax.set_ylabel(\"Risk / Label\")\n    ax.set_title(f\"Patient {patient_id} — past nowcast + future forecast\")\n    ax.legend(loc=\"upper left\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d %H:%M\"))\n    fig.autofmt_xdate(); fig.tight_layout()\n    if out_png:\n        fig.savefig(out_png, dpi=200); print(f\"🖼️ Saved timeline → {out_png}\")\n    plt.show()\n\ndef plot_nowcast_vs_forecast_h1(df_nowcast, df_forecast, patient_id, out_png=None):\n    f1 = df_forecast[(df_forecast[\"patientID\"]==patient_id) & (df_forecast[\"horizon\"]==1)].copy()\n    f1 = f1.rename(columns={\"forecast_hour\":\"hour\", \"y_prob\":\"y_prob_forecast_h1\"})\n    past = df_nowcast[df_nowcast[\"patientID\"]==patient_id][[\"hour\",\"y_prob\",\"y_true\"]].copy()\n\n    # --- force both keys to the same dtype/timezone ---\n    past[\"hour\"] = _to_naive_utc(past[\"hour\"])\n    f1[\"hour\"]   = _to_naive_utc(f1[\"hour\"])\n\n    j = past.merge(f1[[\"hour\",\"y_prob_forecast_h1\"]], on=\"hour\", how=\"inner\").sort_values(\"hour\")\n    if j.empty:\n        print(\"[INFO] No overlap between nowcast timeline and horizon-1 forecasts for this patient.\"); return\n    fig, ax = plt.subplots(figsize=(10,4))\n    ax.plot(j[\"hour\"], j[\"y_prob\"], lw=2, label=\"Nowcast prob (test)\")\n    ax.plot(j[\"hour\"], j[\"y_prob_forecast_h1\"], lw=2, linestyle=\"--\", label=\"Forecast@+1h prob\")\n    ax.step(j[\"hour\"], j[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(\"Hour\"); ax.set_ylabel(\"Risk / Label\")\n    ax.set_title(f\"Patient {patient_id} — nowcast vs forecast@+1h\")\n    ax.legend(loc=\"upper left\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d %H:%M\"))\n    fig.autofmt_xdate(); fig.tight_layout()\n    if out_png:\n        fig.savefig(out_png, dpi=200); print(f\"🖼️ Saved compare@+1h → {out_png}\")\n    plt.show()\n\n# -------------------------------------------------\n# 7) Threshold calibration on VAL (per horizon)\n# -------------------------------------------------\ndef _best_thr_by_fbeta(y, p, beta=1.0, thr_min=0.10, thr_max=0.70):\n    prec, rec, thr = precision_recall_curve(y, p)\n    prec, rec, thr = prec[:-1], rec[:-1], thr\n    mask = np.isfinite(thr) & (thr >= thr_min) & (thr <= thr_max)\n    if not mask.any():\n        return 0.50\n    fb = (1+beta**2) * (prec[mask]*rec[mask]) / (beta**2*prec[mask] + rec[mask] + 1e-9)\n    return float(thr[mask][np.nanargmax(fb)])\n\ndef calibrate_horizon_thresholds_on_val(model, data, hourly, k=12, beta=1.5, method=\"ema\"):\n    vals = []\n    val_mask = hourly[\"Split\"].astype(str).str.lower()==\"val\"\n    val_ids = sorted(hourly.loc[val_mask, \"patientID\"].unique())\n    for pid in val_ids:\n        df_fc_val = rolling_forecast_patient(\n            model, data, hourly, patient_id=pid, k=k, split=\"val\",\n            threshold=0.50, method=method  # temporary; only need probs\n        )\n        if not df_fc_val.empty:\n            vals.append(df_fc_val)\n    if not vals:\n        return {}\n    allv = pd.concat(vals, ignore_index=True)\n    out = {}\n    for h, g in allv.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        if len(np.unique(y)) < 2:\n            continue\n        out[h] = _best_thr_by_fbeta(y, p, beta=beta, thr_min=0.10, thr_max=0.70)\n    return out\n\n# ======================\n# 8) Run: load, forecast, plot\n# ======================\n# Preconditions\nif \"hourly\" not in globals() or \"data\" not in globals():\n    raise RuntimeError(\"hourly/data not found. Please run your feature builder and sequence builder cells first.\")\n\nCKPT_PATH, _best_method, _best_model = pick_best_checkpoint(OUT_RESULTS_CSV, \"checkpoints\")\nmodel = tf.keras.models.load_model(CKPT_PATH, custom_objects={\"loss\": focal_loss, \"focal_loss\": focal_loss})\n\n# (b) Threshold: VAL-optimal or manual\nif AUTO_USE_VAL_OPTIMAL_THR:\n    thr_val = get_val_optimal_threshold(OUT_RESULTS_CSV, _best_method, _best_model)\n    THRESH   = float(thr_val) if (thr_val is not None) else float(THRESH_MANUAL)\n    if thr_val is None:\n        print(f\"[INFO] Using manual THRESH={THRESH_MANUAL:.2f} (VAL-optimal not found).\")\n    else:\n        print(f\"✅ Using VAL-optimal THRESH={THRESH:.2f}\")\nelse:\n    THRESH = float(THRESH_MANUAL)\n    print(f\"ℹ️ Using manual THRESH={THRESH:.2f}\")\n\n# (c) Nowcast on TEST + suggestions\ndf_test_nowcast = predict_split_prob_df(model, data, hourly, split=\"test\", threshold=THRESH)\nif OVERRIDE_PATIENT_ID is not None and OVERRIDE_PATIENT_ID in set(df_test_nowcast[\"patientID\"].unique()):\n    suggested = [OVERRIDE_PATIENT_ID]\nelse:\n    suggested = suggest_patients_for_plots(df_test_nowcast, strategy=PATIENT_SELECTION_STRATEGY,\n                                           top_n=TOP_N_PATIENTS_TO_PLOT)\nprint(\"📌 Suggested patientID(s) to plot:\", suggested)\n\n# (d) Optional: learn per-horizon thresholds on VAL\nHORIZON_THR = None\nif USE_HORIZON_THRESHOLDS:\n    HORIZON_THR = calibrate_horizon_thresholds_on_val(\n        model, data, hourly, k=K_HOURS, beta=FBETA_FOR_CAL, method=FORECAST_METHOD\n    )\n    print(\"Horizon thresholds (VAL):\", HORIZON_THR)\n\n# (e) Forecast & plot\nfor pid in suggested:\n    print(f\"\\n=== Forecasting patient {pid} | K={K_HOURS}, method={FORECAST_METHOD} ===\")\n    thr_to_use = (HORIZON_THR if (isinstance(HORIZON_THR, dict) and len(HORIZON_THR)>0) else THRESH)\n    df_fc = rolling_forecast_patient(model, data, hourly, patient_id=pid, k=K_HOURS,\n                                     split=\"test\", threshold=thr_to_use,\n                                     method=FORECAST_METHOD, ema_alpha=EMA_ALPHA, lin_steps=LIN_STEPS)\n    print(\"Forecast rows:\", len(df_fc))\n\n    # Optional N-of-M smoothing\n    if APPLY_NOFM and not df_fc.empty:\n        df_fc = df_fc.groupby(\"patientID\", group_keys=False).apply(apply_n_of_m_rule, n=NOFM_N, m=NOFM_M)\n\n    # Metrics\n    hz_base = metrics_by_horizon(df_fc)\n    print(\"Horizon metrics (base decision):\\n\", hz_base)\n    if APPLY_NOFM and \"y_pred_nofm\" in df_fc.columns:\n        hz_nofm = metrics_by_horizon_on_col(df_fc, col=\"y_pred_nofm\")\n        print(\"Horizon metrics (N-of-M decision):\\n\", hz_nofm)\n\n    # Plots\n    outA = os.path.join(SAVE_DIR, f\"nowcast_plus_forecast_patient_{pid}.png\")\n    plot_nowcast_and_forecast_timeline(df_test_nowcast, df_fc, patient_id=pid,\n                                       hours_back=72, threshold=thr_to_use, out_png=outA)\n    outB = os.path.join(SAVE_DIR, f\"nowcast_vs_forecast_h1_patient_{pid}.png\")\n    plot_nowcast_vs_forecast_h1(df_test_nowcast, df_fc, patient_id=pid, out_png=outB)\n\nprint(\"✅ Done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# Leak-safe Ramadan features + Balanced LSTM\n# (hourly builder + sequences + training utilities + significance testing)\n# ==============================================\nimport os, time, warnings, random, re\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\nfrom scipy.stats import norm  # <-- added\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nSTATIC_CSV      = \"/kaggle/input/hmc-model-static-variables/outcome_static.csv\"\nVISIT_WIDE_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_wide_by_variable.csv\"\nVISIT_LONG_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_long.csv\"\n\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree.npz\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\nOUT_PLOTS_PNG   = \"/kaggle/working/combined_roc_pr_curves.png\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0\nMIN_CGM_PER_H = 4\nSEQ_LEN       = 24\n\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n    \"pc1_activity_energy\",\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n)\n\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01\nRESAMPLE_METHODS = [\"none\",\"oversample_seq\",\"undersample_seq\",\"smote\",\"smoteenn\",\"smotetomek\"]\nUSE_STATIC_INPUT = True\n\n# --------------------\n# Utilities (robust column picking)\n# --------------------\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_global_seeds(RANDOM_STATE)\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef safe_encode_gender(series):\n    if series.dtype == \"object\":\n        return (series.str.strip().str.lower().map({\"male\":1, \"m\":1, \"female\":0, \"f\":0}))\n    return pd.to_numeric(series, errors=\"coerce\")\n\ndef split_patients(unique_pids, test_size=0.2, val_size=0.1, random_state=RANDOM_STATE):\n    train_pids, test_pids = train_test_split(unique_pids, test_size=test_size, random_state=random_state)\n    val_fraction = val_size / max(1e-9, (1.0 - test_size))\n    train_pids, val_pids = train_test_split(train_pids, test_size=val_fraction, random_state=random_state)\n    return np.array(train_pids), np.array(val_pids), np.array(test_pids)\n\ndef _normalize_date(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    return s.dt.normalize()\n\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef _pick_col_flex(df: pd.DataFrame, preferred=None, required=False, name=\"\", must_contain_all=None, any_contains=None):\n    cols = list(df.columns)\n    norm_map = {c: _norm_col(c) for c in cols}\n    if preferred:\n        lower_pref = {str(p).strip().lower(): p for p in preferred}\n        for c in cols:\n            if str(c).strip().lower() in lower_pref:\n                return c\n    if preferred:\n        pref_norm = {_norm_col(p): p for p in preferred}\n        for c, n in norm_map.items():\n            if n in pref_norm:\n                return c\n    cands = []\n    for c, n in norm_map.items():\n        ok = True\n        if must_contain_all:\n            for tok in must_contain_all:\n                if _norm_col(tok) not in n:\n                    ok = False; break\n        if ok and any_contains:\n            if not any(_norm_col(tok) in n for tok in any_contains):\n                ok = False\n        if ok: cands.append(c)\n    if cands:\n        def _priority(col: str):\n            n = norm_map[col]\n            starts_pid = n.startswith(\"patientid\")\n            has_pid    = \"patientid\" in n\n            return (-(starts_pid or has_pid), len(n))\n        cands.sort(key=_priority)\n        return cands[0]\n    if required:\n        raise KeyError(f\"Required column not found for {name}. preferred={preferred} | must_contain_all={must_contain_all} | any_contains={any_contains}. Available: {cols}\")\n    return None\n\ndef _pick_patient_col(df: pd.DataFrame) -> str:\n    preferred = [\"patientID\",\"patientId\",\"PatientID (Huawei Data)\",\"subject_id\",\"patid\",\"pid\",\"id\",\"huaweiid\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"patientID\",\n                          must_contain_all=[\"id\"], any_contains=[\"patient\",\"subject\",\"pat\",\"huawei\"])\n\ndef _pick_date_col(df: pd.DataFrame) -> str:\n    preferred = [\"date\",\"visit_date\",\"Date\",\"day\",\"timestamp\",\"Visit Date\",\"date_of_visit\",\"start\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"date\",\n                          any_contains=[\"date\",\"visit\",\"day\",\"timestamp\",\"start\"])\n\ndef _pick_variable_col(df: pd.DataFrame) -> str:\n    preferred = [\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"variable\",\n                          any_contains=[\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"])\n\ndef _pick_value_col(df: pd.DataFrame) -> str:\n    preferred = [\"value\",\"val\",\"measure_value\",\"reading\",\"amount\",\"score\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"value\",\n                          any_contains=[\"value\",\"val\",\"measurevalue\",\"reading\",\"amount\",\"score\"])\n\n# ---------------------------\n# Loaders for external files\n# ---------------------------\ndef load_static_df(static_csv=STATIC_CSV, needed=STATIC_COLS):\n    if not static_csv or not os.path.exists(static_csv):\n        print(\"⚠️ Static CSV not found; static features will be zero-filled.\")\n        return None\n    df = pd.read_csv(static_csv)\n    pid_col = _pick_patient_col(df)\n    df = df.rename(columns={pid_col:\"patientID\"})\n    keep = [\"patientID\"] + [c for c in needed if c in df.columns]\n    df = df[keep].drop_duplicates(subset=[\"patientID\"]).copy()\n    if \"Gender\" in df.columns:\n        df[\"Gender\"] = safe_encode_gender(df[\"Gender\"])\n    for c in keep:\n        if c != \"patientID\":\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    print(f\"ℹ️ static: using patientID column = '{pid_col}'\")\n    return df\n\ndef load_visit_df(visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV, needed=VISIT_COLS):\n    if visit_wide_csv and os.path.exists(visit_wide_csv):\n        wide = pd.read_csv(visit_wide_csv)\n        pid_col  = _pick_patient_col(wide)\n        date_col = _pick_date_col(wide)\n        wide = wide.rename(columns={pid_col:\"patientID\", date_col:\"date\"})\n        wide[\"date\"] = _normalize_date(wide[\"date\"])\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"ℹ️ visit-wide: patientID='{pid_col}', date='{date_col}', kept={keep[2:]}\")\n            return wide[keep].copy()\n        else:\n            print(\"⚠️ VISIT_WIDE_CSV found but none of the needed visit columns present; will try LONG if available.\")\n    if visit_long_csv and os.path.exists(visit_long_csv):\n        long = pd.read_csv(visit_long_csv)\n        pid_col   = _pick_patient_col(long)\n        date_col  = _pick_date_col(long)\n        var_col   = _pick_variable_col(long)\n        value_col = _pick_value_col(long)\n        long = long.rename(columns={pid_col:\"patientID\", date_col:\"date\", var_col:\"variable\", value_col:\"value\"})\n        long[\"date\"] = _normalize_date(long[\"date\"])\n        wide = (long\n                .pivot_table(index=[\"patientID\",\"date\"], columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n                .reset_index())\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"ℹ️ visit-long: patientID='{pid_col}', date='{date_col}', variables matched={keep[2:]}\")\n            return wide[keep].copy()\n        print(\"⚠️ VISIT_LONG_CSV present but none of the needed variables were found in the pivot.\")\n    print(\"⚠️ No usable visit CSVs found; visit features will be zero-filled.\")\n    return None\n\n# ----------------------------------------------------------------\n# Part A — Build hourly Ramadan features and leak-safe transforms\n# ----------------------------------------------------------------\ndef build_hourly_features_with_leak_safe_transforms(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n    static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n    df = pd.read_csv(in_csv)\n    if \"patientID\" not in df.columns:\n        pid_col = _pick_patient_col(df)\n        df = df.rename(columns={pid_col: \"patientID\"})\n        print(f\"ℹ️ intraday: using patientID column = '{pid_col}'\")\n\n    start_col = \"start\" if \"start\" in df.columns else _pick_date_col(df)\n    df[start_col] = to_dt(df[start_col])\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[start_col].dt.date)\n    df[\"hour\"]       = df[start_col].dt.floor(\"h\")\n    df[\"hour_of_day\"]= df[\"hour\"].dt.hour\n\n    df = ensure_numeric(df)\n    df = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"❌ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(\n                       cgm_min=(\"cgm\",\"min\"),\n                       cgm_max=(\"cgm\",\"max\"),\n                       cgm_mean=(\"cgm\",\"mean\"),\n                       cgm_std=(\"cgm\",\"std\")\n                   )\n                   .sort_values([\"patientID\",\"hour\"])\n                   .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean().fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    pids = hourly[\"patientID\"].dropna().unique()\n    train_p, val_p, test_p = split_patients(pids, test_size=test_size, val_size=val_size, random_state=random_state)\n    hourly[\"Split\"] = np.where(hourly[\"patientID\"].isin(train_p), \"train\",\n                        np.where(hourly[\"patientID\"].isin(val_p), \"val\", \"test\"))\n\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"] == \"train\"\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    def _apply_cgm_pca(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n    hourly = _apply_cgm_pca(hourly)\n\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(\n            scal_life.transform(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        )\n        X_all = scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        Z_all = pca_life.transform(X_all)\n        hourly[\"pc1_activity_energy\"] = Z_all[:,0]\n        hourly[\"pc2_physiology\"]      = Z_all[:,1]\n        hourly[\"pc3_sleep_rest\"]      = Z_all[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    visit_df = load_visit_df(visit_wide_csv, visit_long_csv, VISIT_COLS)\n    hourly[\"date\"] = hourly[\"hour\"].dt.normalize()\n    if visit_df is not None:\n        visit_df[\"date\"] = pd.to_datetime(visit_df[\"date\"], errors=\"coerce\").dt.normalize()\n        visit_df = visit_df[(visit_df[\"date\"] >= RAMADAN_START) & (visit_df[\"date\"] <= RAMADAN_END)].copy()\n        hourly = hourly.merge(visit_df, on=[\"patientID\",\"date\"], how=\"left\")\n    for c in VISIT_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    static_df = load_static_df(static_csv, STATIC_COLS)\n    if static_df is not None:\n        hourly = hourly.merge(static_df, on=\"patientID\", how=\"left\")\n    for c in STATIC_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(cast := hourly[c], errors=\"coerce\").fillna(0.0)\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(out_csv, index=False)\n    print(f\"✅ Saved hourly features (leak-safe) → {out_csv}\")\n\n    return hourly, (train_p, val_p, test_p)\n\n# ---------------------------------------------------------------\n# Part B — Build sequences (optionally with per-patient static)\n# ---------------------------------------------------------------\ndef build_sequences_by_split(hourly, splits, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n                             static_cols=STATIC_COLS, scale_features=True):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float).fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n                if static_mat is not None and pid in static_mat.index:\n                    X_stat.append(static_mat.loc[pid].values.astype(float))\n        X_seq = np.array(X_seq); y = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size==0: return X\n            n = X.shape[0]; return seq_scaler.transform(X.reshape(-1, n_f)).reshape(n, SEQ_LEN, n_f)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size>0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size==0: return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"✅ Sequences built | train={Xtr_s.shape}, val={Xva_s.shape}, test={Xte_s.shape}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\nTHR_MIN, THR_MAX = THR_MIN, THR_MAX\n\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask])); idx = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment_with_static(X_seq, X_stat, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0:\n        return X_seq, X_stat, y\n    noise = np.random.normal(0, sigma, X_seq.shape)\n    X_seq_aug = np.vstack([X_seq, X_seq + noise])\n    y_aug     = np.hstack([y, y])\n    if X_stat is not None:\n        X_stat_aug = np.vstack([X_stat, X_stat])\n    else:\n        X_stat_aug = None\n    return X_seq_aug, X_stat_aug, y_aug\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE, return_index=False, allow_smote=True):\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n    base_idx = np.arange(n)\n\n    if method == \"none\":\n        return (X, y, base_idx) if return_index else (X, y)\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return (X, y, base_idx) if return_index else (X, y)\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        Xr, yr = X[keep], y[keep]\n        return (Xr, yr, keep) if return_index else (Xr, yr)\n\n    if not allow_smote:\n        print(f\"⚠️ {method} disabled when static input is used; falling back to 'none'.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"⚠️ Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    Xf = X.reshape(n, -1)\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n\n    Xr = Xr.reshape(-1, T, F)\n    return (Xr, yr, None) if return_index else (Xr, yr)\n\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state=RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0:\n        return (X_test, y_test, (None if X_stat is None else X_stat))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ------------------------------------------------------\n# Model builders (supports seq-only or seq+static)\n# ------------------------------------------------------\ndef make_model(seq_len, n_seq_f, n_stat_f=0, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n        x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x)\n        x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x);                    x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L1\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l1(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l1(1e-5))(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x);                          x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(32, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(32, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(32, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# ==========================\n# Significance / Uncertainty\n# ==========================\ndef delong_bootstrap_auc(y_true, p1, p2, n_boot=2000, random_state=42):\n    \"\"\"\n    Bootstrap test for ΔAUC = AUC(p1) - AUC(p2).\n    Returns (delta_auc, se, z, p_value_two_sided).\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    y_true = np.asarray(y_true).astype(int).ravel()\n    p1 = np.asarray(p1).astype(float).ravel()\n    p2 = np.asarray(p2).astype(float).ravel()\n\n    diffs = []\n    n = len(y_true)\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, n)\n        yb, p1b, p2b = y_true[idx], p1[idx], p2[idx]\n        try:\n            diffs.append(roc_auc_score(yb, p1b) - roc_auc_score(yb, p2b))\n        except ValueError:\n            continue\n    diffs = np.array(diffs, dtype=float)\n    delta = float(np.nanmean(diffs))\n    se    = float(np.nanstd(diffs, ddof=1))\n    z     = 0.0 if se == 0.0 else delta / se\n    pval  = 2.0 * (1.0 - norm.cdf(abs(z)))\n    return delta, se, z, pval\n\ndef bootstrap_ci_auc(y_true, p, n_boot=2000, alpha=0.05, random_state=42):\n    \"\"\"\n    Percentile bootstrap CI for ROC-AUC. Returns (auc_hat, [lo, hi]).\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    y_true = np.asarray(y_true).astype(int).ravel()\n    p = np.asarray(p).astype(float).ravel()\n\n    stats = []\n    n = len(y_true)\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, n)\n        yb, pb = y_true[idx], p[idx]\n        try:\n            stats.append(roc_auc_score(yb, pb))\n        except ValueError:\n            continue\n    stats = np.array(stats, dtype=float)\n    auc_hat = float(roc_auc_score(y_true, p))\n    lo = float(np.nanpercentile(stats, 2.5))\n    hi = float(np.nanpercentile(stats, 97.5))\n    return auc_hat, [lo, hi]\n\n# ------------------------------------------------------\n# Training runner (VAL for threshold; TEST for final)\n# ------------------------------------------------------\ndef run_balanced_lstm_pipeline(data,\n                               arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE,\n                               results_csv=OUT_RESULTS_CSV,\n                               plots_png=OUT_PLOTS_PNG):\n\n    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n    os.makedirs(os.path.dirname(plots_png), exist_ok=True)\n    os.makedirs(\"checkpoints\", exist_ok=True)\n\n    Xtr, Xtr_stat, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Xva_stat, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n    Xte, Xte_stat, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"X_stat\"],  data[\"test\"][\"y\"]\n\n    Xtr, Xtr_stat, ytr = augment_with_static(Xtr, Xtr_stat, ytr, sigma=AUGMENT_SIGMA)\n\n    Xte_bal, yte_bal, Xte_stat_bal = make_balanced_test(Xte, yte, X_stat=Xte_stat)\n\n    results     = {}\n    roc_curves  = {}\n    pr_curves   = {}\n\n    # For post-hoc significance tests\n    prob_store = {}  # (method, arch, split) -> (y_true, y_prob)\n\n    allow_smote = (Xtr_stat is None or not USE_STATIC_INPUT)\n\n    def train_eval_one(method_name, arch_name):\n        nonlocal Xtr, ytr, Xtr_stat\n\n        Xrs, yrs, idx_map = seq_resample(Xtr, ytr, method=method_name, random_state=random_state,\n                                         return_index=True, allow_smote=allow_smote)\n        if Xtr_stat is not None and USE_STATIC_INPUT:\n            Xrs_stat = Xtr_stat if idx_map is None else Xtr_stat[idx_map]\n        else:\n            Xrs_stat = None\n\n        seq_len, n_seq_f = Xrs.shape[1], Xrs.shape[2]\n        n_stat_f = 0 if (Xrs_stat is None or not USE_STATIC_INPUT) else Xrs_stat.shape[1]\n        model = make_model(seq_len, n_seq_f, n_stat_f=n_stat_f, arch=arch_name, lr=1e-3)\n\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n        ckpt_path = f\"checkpoints/{method_name}__{arch_name}.h5\"\n        cp = ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_loss\", verbose=0)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xrs, Xrs_stat], yrs,\n                      validation_data=([Xva, Xva_stat], yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict([Xrs, Xrs_stat], verbose=0).ravel()\n            p_va  = model.predict([Xva, Xva_stat], verbose=0).ravel()\n            p_te  = model.predict([Xte, Xte_stat], verbose=0).ravel()\n            p_teB = model.predict([Xte_bal, Xte_stat_bal], verbose=0).ravel() if Xte_stat_bal is not None else model.predict(Xte_bal, verbose=0).ravel()\n        else:\n            model.fit(Xrs, yrs,\n                      validation_data=(Xva, yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict(Xrs, verbose=0).ravel()\n            p_va  = model.predict(Xva, verbose=0).ravel()\n            p_te  = model.predict(Xte, verbose=0).ravel()\n            p_teB = model.predict(Xte_bal, verbose=0).ravel()\n\n        # thresholds on VAL\n        try:\n            fpr_va, tpr_va, thr_roc_va = roc_curve(yva, p_va); auc_roc = auc(fpr_va, tpr_va)\n        except ValueError:\n            fpr_va, tpr_va, thr_roc_va, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden_va = tpr_va - fpr_va\n        t_roc, _ = _best_threshold_in_range(thr_roc_va, youden_va, thr_min, thr_max)\n\n        prec_va, rec_va, thr_pr_va = precision_recall_curve(yva, p_va)\n        f1s_va = 2*prec_va[:-1]*rec_va[:-1] / (prec_va[:-1]+rec_va[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr_va, f1s_va, thr_min, thr_max)\n        ap_val  = average_precision_score(yva, p_va)\n\n        roc_curves[(method_name, arch_name)] = (fpr_va, tpr_va, auc_roc)\n        pr_curves[(method_name, arch_name)]  = (rec_va, prec_va, ap_val)\n        print(f\"📌 [{method_name}/{arch_name}] VAL thresholds → Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_va  = (p_va  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__train\"]        = evaluate_full_metrics(yrs,     yhat_tr,  p_tr)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__val\"]          = evaluate_full_metrics(yva,     yhat_va,  p_va)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__test\"]         = evaluate_full_metrics(yte,     yhat_te,  p_te)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__testBalanced\"] = evaluate_full_metrics(yte_bal, yhat_teB, p_teB)\n\n        # --- store probabilities for A/B significance (threshold-free AUC) ---\n        prob_store[(method_name, arch_name, \"test\")]         = (yte,     p_te)\n        prob_store[(method_name, arch_name, \"testBalanced\")] = (yte_bal, p_teB)\n\n    # Loop: resampling methods × architectures\n    for METHOD in resample_methods:\n        if METHOD in {\"smote\",\"smoteenn\",\"smotetomek\"} and (data[\"train\"][\"X_stat\"] is not None and USE_STATIC_INPUT):\n            print(f\"⏭️  Skipping {METHOD} (static input enabled).\")\n            continue\n        print(f\"\\n🔁 Resampling: {METHOD} | train y-dist = {Counter(data['train']['y'])}\")\n        for ARCH in arch_list:\n            train_eval_one(METHOD, ARCH)\n\n    # --- Plots (validation curves)\n    plt.figure(figsize=(14,6))\n    plt.subplot(1,2,1)\n    for (meth, arch), (fpr, tpr, auc_roc) in roc_curves.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{arch} (VAL AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (Validation)'); plt.legend(fontsize=8)\n    plt.subplot(1,2,2)\n    for (meth, arch), (rec, prec, ap) in pr_curves.items():\n        plt.plot(rec, prec, label=f'{meth}/{arch} (VAL AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR (Validation)'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(plots_png, dpi=300); plt.show()\n    print(f\"🖼️ Saved plots → {plots_png}\")\n\n    # --- Results CSV\n    results_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"Key\"})\n    k = results_df[\"Key\"].str.strip()\n    results_df[\"Split\"]  = np.where(k.str.endswith(\"__train\"), \"train\",\n                             np.where(k.str.endswith(\"__val\"), \"val\",\n                             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                             np.where(k.str.endswith(\"__test\"), \"test\", np.nan))))\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"]    = parts.str[0]\n    results_df[\"Model\"]     = parts.str[1]\n    results_df[\"Threshold\"] = pd.to_numeric(parts.str[2].str.replace(\"thr_\",\"\", regex=False), errors=\"coerce\")\n    results_df.round(6).to_csv(results_csv, index=False)\n    print(f\"📑 Saved results → {results_csv}\")\n\n    # ================================\n    # Pairwise AUC significance (A/B)\n    # ================================\n    pairs_to_compare = [\n        ((\"none\",\"LSTM_100\"), (\"oversample_seq\",\"LSTM_100\")),\n        ((\"none\",\"LSTM_100\"), (\"undersample_seq\",\"BiLSTM\")),\n        ((\"oversample_seq\",\"LSTM_100\"), (\"undersample_seq\",\"BiLSTM\")),\n    ]\n    splits_to_use = [\"test\", \"testBalanced\"]\n\n    sig_rows = []\n    for split_name in splits_to_use:\n        for (A, B) in pairs_to_compare:\n            methA, archA = A\n            methB, archB = B\n            keyA = (methA, archA, split_name)\n            keyB = (methB, archB, split_name)\n            if keyA not in prob_store or keyB not in prob_store:\n                continue\n            (yA, pA) = prob_store[keyA]\n            (yB, pB) = prob_store[keyB]\n            y_true = yA  # same split -> same ground truth\n            delta, se, z, p = delong_bootstrap_auc(y_true, pA, pB, n_boot=2000, random_state=random_state)\n            aucA, ciA = bootstrap_ci_auc(y_true, pA, n_boot=2000, alpha=0.05, random_state=random_state)\n            aucB, ciB = bootstrap_ci_auc(y_true, pB, n_boot=2000, alpha=0.05, random_state=random_state)\n            sig_rows.append({\n                \"Split\": split_name,\n                \"ModelA\": f\"{methA}/{archA}\",\n                \"ModelB\": f\"{methB}/{archB}\",\n                \"AUC_A\": aucA, \"AUC_A_CI95_L\": ciA[0], \"AUC_A_CI95_U\": ciA[1],\n                \"AUC_B\": aucB, \"AUC_B_CI95_L\": ciB[0], \"AUC_B_CI95_U\": ciB[1],\n                \"Delta_AUC\": delta, \"SE_Delta\": se, \"Z\": z, \"P_value\": p\n            })\n\n    sig_df = pd.DataFrame(sig_rows)\n    out_sig_csv = (os.path.join(os.path.dirname(results_csv), \"auc_significance.csv\")\n                   if os.path.dirname(results_csv) else \"auc_significance.csv\")\n    sig_df.to_csv(out_sig_csv, index=False)\n    print(f\"📑 Saved AUC significance table → {out_sig_csv}\")\n    if not sig_df.empty:\n        print(sig_df.head(10).to_string(index=False))\n\n    return results_df\n\n# --------------------\n# Run end-to-end\n# --------------------\nif __name__ == \"__main__\":\n    hourly, splits = build_hourly_features_with_leak_safe_transforms(\n        in_csv=CSV_INTRADAY_WITH_VISITS,\n        out_csv=OUT_HOURLY_CSV,\n        min_cgm_per_hour=MIN_CGM_PER_H,\n        test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n        static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n    )\n\n    data = build_sequences_by_split(\n        hourly, splits,\n        seq_len=SEQ_LEN,\n        seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n        static_cols=STATIC_COLS,\n        scale_features=True\n    )\n\n    np.savez_compressed(\n        OUT_SEQ_NPZ,\n        Xtr=data[\"train\"][\"X_seq\"],  Xtr_stat=(data[\"train\"][\"X_stat\"] if data[\"train\"][\"X_stat\"] is not None else np.empty((0,0))),\n        ytr=data[\"train\"][\"y\"],\n        Xva=data[\"val\"][\"X_seq\"],    Xva_stat=(data[\"val\"][\"X_stat\"] if data[\"val\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yva=data[\"val\"][\"y\"],\n        Xte=data[\"test\"][\"X_seq\"],   Xte_stat=(data[\"test\"][\"X_stat\"] if data[\"test\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yte=data[\"test\"][\"y\"],\n        seq_features_used=np.array(data[\"seq_features_used\"], dtype=object),\n        static_features_used=np.array(data[\"static_features_used\"], dtype=object)\n    )\n    print(f\"💾 Saved sequences → {OUT_SEQ_NPZ}\")\n\n    results_df = run_balanced_lstm_pipeline(\n        data,\n        arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n        resample_methods=RESAMPLE_METHODS,\n        thr_min=THR_MIN, thr_max=THR_MAX,\n        random_state=RANDOM_STATE,\n        results_csv=OUT_RESULTS_CSV,\n        plots_png=OUT_PLOTS_PNG\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Cross-Phase Validation (Leak-safe)\n#   Train: Ramadan  → Test: Shawwal\n#   Train: Shawwal  → Test: Ramadan\n# Models: LSTM_100, BiLSTM, LSTM_50, LSTM_25_L2\n# ============================================================\n\nimport os, re, random, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, mean_squared_error\n)\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\n\n# --------------------------\n# CONFIG\n# --------------------------\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE); tf.random.set_seed(RANDOM_STATE)\n\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\n# If you don't have static/visit files ready, leave them None\nSTATIC_CSV      = None\nVISIT_WIDE_CSV  = None\nVISIT_LONG_CSV  = None\n\nRAMADAN_START  = pd.to_datetime(\"2023-03-22\"); RAMADAN_END  = pd.to_datetime(\"2023-04-19\")\nSHAWWAL_START  = pd.to_datetime(\"2023-04-20\"); SHAWWAL_END  = pd.to_datetime(\"2023-05-19\")\n\nSEQ_LEN       = 24\nHYPO_CUTOFF   = 70.0\nMIN_CGM_PER_H = 4\nVAL_FRACTION  = 0.15  # within *training phase* patients\n\nUSE_STATIC_INPUT = False  # this script runs seq-only (set True if you extend with static branch)\nARCH_LIST = (\"LSTM_100\",\"BiLSTM\",\"LSTM_50\",\"LSTM_25_L2\")\nTHR_MIN, THR_MAX = 0.40, 0.60\n\n# Features (sequence)\nLIFESTYLE_COLS_CANDIDATES = [\"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"]\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\"pc1_activity_energy\"\n)\n\n# --------------------------\n# UTILS\n# --------------------------\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    return {0: float(N/(2.0*n0)), 1: float(N/(2.0*n1))}\n\n# --------------------------\n# DATA: build hourly per window\n# --------------------------\ndef build_hourly_for_window(in_csv, start_date, end_date, min_cgm_per_hour=MIN_CGM_PER_H):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(in_csv)\n    df = pd.read_csv(in_csv)\n\n    # normalize patient column\n    if \"patientID\" not in df.columns:\n        for c in df.columns:\n            if _norm_col(c).startswith(\"patientid\") or _norm_col(c) in {\"patientid\",\"id\",\"huaweiid\"}:\n                df = df.rename(columns={c:\"patientID\"})\n                break\n        if \"patientID\" not in df.columns:\n            raise KeyError(\"patientID column not found.\")\n\n    # time columns\n    time_col = \"start\" if \"start\" in df.columns else (\"timestamp\" if \"timestamp\" in df.columns else None)\n    if time_col is None and \"date\" not in df.columns:\n        raise KeyError(\"No start/timestamp/date column found.\")\n    if time_col is not None:\n        df[time_col] = to_dt(df[time_col])\n        df[\"date\"] = pd.to_datetime(df[time_col].dt.date)\n        df[\"hour\"] = df[time_col].dt.floor(\"h\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n        df[\"hour\"] = df[\"date\"].dt.floor(\"h\")\n\n    df[\"hour_of_day\"] = pd.to_datetime(df[\"hour\"]).dt.hour\n    df = ensure_numeric(df)\n\n    # filter window\n    df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)].copy()\n\n    # CGM present?\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # valid hours\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # hourly summaries\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(cgm_min=(\"cgm\",\"min\"),\n                        cgm_max=(\"cgm\",\"max\"),\n                        cgm_mean=(\"cgm\",\"mean\"),\n                        cgm_std=(\"cgm\",\"std\"))\n                   .sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = pd.to_datetime(hourly[\"hour\"]).dt.hour\n\n    # labels\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # composites\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # lifestyle means (if present)\n    life_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if life_cols:\n        life_hourly = (df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[life_cols].mean().fillna(0.0))\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n    else:\n        for c in [\"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"]:\n            if c not in hourly.columns:\n                hourly[c] = 0.0\n\n    hourly[\"date\"] = pd.to_datetime(pd.to_datetime(hourly[\"hour\"]).dt.date)\n    return hourly\n\n# --------------------------\n# Fit leak-safe PCA (train phase only) and add PCA cols\n# --------------------------\ndef apply_leak_safe_pca(hourly, lifestyle_cols=None):\n    hourly = hourly.copy()\n    assert \"Split\" in hourly.columns, \"hourly must have Split\"\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"].astype(str).str.lower().eq(\"train\")\n\n    # CGM PCA\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=RANDOM_STATE).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    X_all = scal_cgm.transform(hourly[cgm_cols].fillna(0.0))\n    Z_all = pca_cgm.transform(X_all)\n    hourly[\"pca_cgm1\"], hourly[\"pca_cgm2\"], hourly[\"pca_cgm3\"] = Z_all[:,0], Z_all[:,1], Z_all[:,2]\n\n    # Lifestyle PCA (if present)\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        ZL = PCA(n_components=3, random_state=RANDOM_STATE).fit_transform(\n            scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        )\n        hourly[\"pc1_activity_energy\"], hourly[\"pc2_physiology\"], hourly[\"pc3_sleep_rest\"] = ZL[:,0], ZL[:,1], ZL[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n\n    return hourly\n\n# --------------------------\n# Build sequences by split\n# --------------------------\ndef build_sequences(hourly, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing {c}\")\n\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    missing = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing:\n        raise KeyError(f\"Missing seq features: {missing}\")\n\n    train_p = sorted(hourly.loc[hourly[\"Split\"].eq(\"train\"), \"patientID\"].unique())\n    val_p   = sorted(hourly.loc[hourly[\"Split\"].eq(\"val\"),   \"patientID\"].unique())\n    test_p  = sorted(hourly.loc[hourly[\"Split\"].eq(\"test\"),  \"patientID\"].unique())\n\n    def _build_for(pid_list):\n        sub = hourly[hourly[\"patientID\"].isin(pid_list)].sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n        X, y = [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len: continue\n            feats  = grp[list(seq_feature_cols)].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp)-seq_len):\n                X.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n        return (np.array(X), np.array(y).astype(int))\n\n    Xtr, ytr = _build_for(train_p)\n    Xva, yva = _build_for(val_p)\n    Xte, yte = _build_for(test_p)\n\n    # scaler on TRAIN only (sequence features)\n    seq_scaler = None\n    if Xtr.size > 0:\n        F = Xtr.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr.reshape(-1, F))\n        def _scale(X):\n            if X is None or X.size==0: return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n        Xtr = _scale(Xtr); Xva = _scale(Xva); Xte = _scale(Xte)\n\n    return {\n        \"train\": {\"X_seq\": Xtr, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte, \"y\": yte},\n        \"seq_features_used\": list(seq_feature_cols),\n        \"scalers\": {\"seq\": seq_scaler}\n    }\n\n# --------------------------\n# Models\n# --------------------------\ndef make_model(seq_len, n_seq_f, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x); x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x); x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x); x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x); x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    out = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=seq_in, outputs=out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# --------------------------\n# Threshold selection & evaluation\n# --------------------------\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx = np.where(mask)[0][int(np.nanargmax(scores[mask]))]\n        return float(thresholds[idx])\n    # fallback: best overall, then clamp\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max))\n\ndef pick_val_thresholds(y_val, p_val, thr_min=THR_MIN, thr_max=THR_MAX):\n    # Youden's J\n    try:\n        fpr, tpr, thr_roc = roc_curve(y_val, p_val)\n        youden = tpr - fpr\n        t_roc = _best_threshold_in_range(thr_roc, youden, thr_min, thr_max)\n    except Exception:\n        t_roc = 0.50\n    # PR-F1\n    try:\n        prec, rec, thr_pr = precision_recall_curve(y_val, p_val)\n        f1s = 2*prec[:-1]*rec[:-1] / (prec[:-1]+rec[:-1]+1e-9)\n        t_pr = _best_threshold_in_range(thr_pr, f1s, thr_min, thr_max)\n    except Exception:\n        t_pr = 0.50\n    return t_roc, t_pr\n\ndef eval_probs(y_true, y_prob, thresholds=(0.40, 0.50, 0.60)):\n    \"\"\"\n    Evaluate thresholded metrics at specified operating points, plus AUC/PR-AUC/Brier (threshold-free).\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_prob = np.asarray(y_prob).astype(float).ravel()\n\n    try:\n        roc = roc_auc_score(y_true, y_prob)\n    except Exception:\n        roc = np.nan\n    try:\n        pr  = average_precision_score(y_true, y_prob)\n    except Exception:\n        pr  = np.nan\n    brier = mean_squared_error(y_true, y_prob)\n\n    rows = []\n    for t in thresholds:\n        yhat = (y_prob >= float(t)).astype(int)\n        rows.append({\n            \"Threshold\": round(float(t), 2),\n            \"Accuracy\": accuracy_score(y_true, yhat),\n            \"F1_weighted\": f1_score(y_true, yhat, average=\"weighted\", zero_division=0),\n            \"Prec_1\": precision_score(y_true, yhat, pos_label=1, zero_division=0),\n            \"Recall_1\": recall_score(y_true, yhat, pos_label=1, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr, \"Brier\": brier\n        })\n    return pd.DataFrame(rows)\n\n# --------------------------\n# Core runner (one direction)\n# --------------------------\ndef run_cross_phase(train_window, test_window, out_prefix=\"ram_to_sha\"):\n    \"\"\"\n    train_window/test_window: (start_date, end_date)\n    \"\"\"\n    # A) Build hourly tables for both phases\n    hourly_train = build_hourly_for_window(CSV_INTRADAY_WITH_VISITS, *train_window)\n    hourly_test  = build_hourly_for_window(CSV_INTRADAY_WITH_VISITS, *test_window)\n\n    # B) Create patient splits: TRAIN/VAL within train phase; TEST from test phase (no overlap)\n    train_patients_all = sorted(hourly_train[\"patientID\"].unique().tolist())\n    test_patients_all  = sorted(hourly_test[\"patientID\"].unique().tolist())\n\n    # hold-out VAL from train phase (patient-wise)\n    rng = np.random.default_rng(RANDOM_STATE)\n    rng.shuffle(train_patients_all)\n    n_val = max(1, int(len(train_patients_all) * VAL_FRACTION))\n    val_patients = sorted(train_patients_all[:n_val])\n    tr_patients  = sorted(train_patients_all[n_val:])\n\n    # ensure cross-phase TEST patients don't overlap training phase patients\n    test_only = sorted(list(set(test_patients_all) - set(tr_patients) - set(val_patients)))\n    if len(test_only) == 0:\n        # fallback: allow all test phase patients (still leak-free wrt transforms, but same subjects across phases)\n        test_only = test_patients_all\n\n    hourly_train[\"Split\"] = np.where(hourly_train[\"patientID\"].isin(tr_patients), \"train\",\n                              np.where(hourly_train[\"patientID\"].isin(val_patients), \"val\", \"drop\"))\n    hourly_train = hourly_train[hourly_train[\"Split\"] != \"drop\"].copy()\n\n    hourly_test[\"Split\"]  = np.where(hourly_test[\"patientID\"].isin(test_only), \"test\", \"drop\")\n    hourly_test = hourly_test[hourly_test[\"Split\"] != \"drop\"].copy()\n\n    # C) Merge to single hourly dataframe with Split: train/val/test\n    hourly = pd.concat([hourly_train, hourly_test], ignore_index=True)\n\n    # D) Leak-safe PCA (fit on TRAIN only)\n    life_cols_present = [c for c in LIFESTYLE_COLS_CANDIDATES if c in hourly.columns]\n    hourly = apply_leak_safe_pca(hourly, lifestyle_cols=life_cols_present)\n\n    # E) Build sequences\n    data = build_sequences(hourly, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS)\n    Xtr, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"y\"]\n    Xva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"y\"]\n    Xte, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"y\"]\n\n    if any(arr is None or arr.size == 0 for arr in [Xtr, ytr, Xva, yva, Xte, yte]):\n        raise RuntimeError(\"Insufficient sequences in one of the splits. Check coverage or windows.\")\n\n    results_rows = []\n\n    # F) Train/evaluate per architecture\n    for arch in ARCH_LIST:\n        model = make_model(seq_len=Xtr.shape[1], n_seq_f=Xtr.shape[2], arch=arch, lr=1e-3)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=0)\n\n        hist = model.fit(Xtr, ytr,\n                         validation_data=(Xva, yva),\n                         epochs=12, batch_size=64,\n                         callbacks=[es],\n                         class_weight=make_class_weight(ytr),\n                         verbose=0)\n\n        # Predict\n        p_tr = model.predict(Xtr, verbose=0).ravel()\n        p_va = model.predict(Xva, verbose=0).ravel()\n        p_te = model.predict(Xte, verbose=0).ravel()\n\n        # Pick thresholds on VAL\n        t_roc, t_pr = pick_val_thresholds(yva, p_va, thr_min=THR_MIN, thr_max=THR_MAX)\n        eval_df = eval_probs(yte, p_te, thresholds=(THR_MIN, 0.50, THR_MAX, t_roc, t_pr))\n        eval_df.insert(0, \"Model\", arch)\n        eval_df.insert(1, \"Direction\", out_prefix)\n        results_rows.append(eval_df)\n\n    results = pd.concat(results_rows, ignore_index=True)\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    out_csv = f\"/kaggle/working/crossphase_{out_prefix}.csv\"\n    results.to_csv(out_csv, index=False)\n    print(f\"✅ Saved → {out_csv}\")\n    return results\n\n# --------------------------\n# RUN BOTH DIRECTIONS\n# --------------------------\nramadan = (RAMADAN_START, RAMADAN_END)\nshawwal  = (SHAWWAL_START, SHAWWAL_END)\n\nres_ram_to_sha = run_cross_phase(ramadan, shawwal, out_prefix=\"Ramadan_to_Shawwal\")\nres_sha_to_ram = run_cross_phase(shawwal, ramadan, out_prefix=\"Shawwal_to_Ramadan\")\n\n# Combine + pivot for quick view\ncombined = pd.concat([res_ram_to_sha, res_sha_to_ram], ignore_index=True)\ncombined.to_csv(\"/kaggle/working/crossphase_combined.csv\", index=False)\n\n# Produce a compact comparison at VAL-optimized PR-F1 threshold (we pick the row with Threshold == t_pr ~= within [0.40,0.60])\ndef _pick_best_rows(df):\n    # We approximated two VAL-optimal thresholds (t_roc, t_pr) and included both in eval table.\n    # Here we pick the row with the highest F1_weighted per Model/Direction.\n    key_cols = [\"Direction\",\"Model\"]\n    best = (df.sort_values([\"Direction\",\"Model\",\"F1_weighted\"], ascending=[True, True, False])\n              .groupby(key_cols, as_index=False)\n              .head(1)\n              .reset_index(drop=True))\n    return best[[\"Direction\",\"Model\",\"Threshold\",\"ROC_AUC\",\"PR_AUC\",\"F1_weighted\",\"Recall_1\",\"Prec_1\",\"Brier\"]]\n\nsummary = _pick_best_rows(combined)\nsummary.to_csv(\"/kaggle/working/crossphase_summary_best.csv\", index=False)\nprint(\"🧾 Summary (best per Model/Direction):\")\nprint(summary.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:12:22.334284Z","iopub.execute_input":"2025-10-29T08:12:22.334997Z"}},"outputs":[{"name":"stderr","text":"2025-10-29 08:12:25.218950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761725545.472809      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761725545.544014      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1761725576.557517      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nI0000 00:00:1761725582.008243     110 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"}],"execution_count":null}]}